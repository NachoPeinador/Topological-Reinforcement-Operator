\documentclass[12pt, a4paper]{article}

% --- PAQUETES ---
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\usepackage{booktabs} % Para tablas de alta calidad
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{caption}

% --- DEFINICIÓN DE COLORES Y ESTILOS ---
\definecolor{oxfordblue}{RGB}{0, 33, 71}
\hypersetup{
    colorlinks=true,
    linkcolor=oxfordblue,
    urlcolor=oxfordblue,
}
\titleformat{\section}{\large\bfseries\color{oxfordblue}}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}

% --- METADATOS DEL DOCUMENTO ---
\title{Refuerzo Topológico en Redes de Grafos Neuronales para la Emulación de la Consolidación de Engramas}

\author{José Ignacio Peinador Sala \\ \textit{Investigador Independiente}}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
\noindent La consolidación de engramas de memoria es un proceso neurobiológico clave y un reto central en la neurociencia computacional. Este trabajo introduce el Operador de Refuerzo Topológico (ORT): un mecanismo post-hoc que consolida engramas funcionales en Redes Neuronales de Grafos (GNNs) previamente calibradas, reforzando selectivamente los nodos de mayor centralidad. Validamos el método en múltiples redes de citaciones, donde alcanza una \textbf{recuperación de memoria asociativa perfecta (100\%)} en Pubmed y \textbf{robusta (>92\%)} en Cora. Finalmente, mostramos que los principios topológicos del ORT son consistentes con la estructura y dinámica de un conectoma humano a gran escala: no solo identificamos un núcleo de ``super-hubs'' (5\% superior) con centralidad $\sim$10.3 veces mayor según Grado y $\sim$14.9 veces según PageRank, sino que demostramos experimentalmente que este núcleo, dentro del propio conectoma, permite una recuperación de memoria funcional perfecta (Recall = 1.0, Tasa de recuperación de olvidados = 1.0). El ORT se establece así como un principio reproducible, escalable y biológicamente plausible para la inducción de memoria funcional, sentando las bases para nuevos paradigmas en neurociencia computacional e inteligencia artificial.
\end{abstract}



\section{Introducción}
La memoria posee un sustrato físico en el cerebro conocido como ``engrama'': un conjunto de neuronas que se activan de forma coordinada para codificar una experiencia \cite{josselyn2020}. Para persistir, un engrama debe someterse a un proceso de \textbf{consolidación}, durante el cual las conexiones sinápticas entre las neuronas del engrama se fortalecen y estabilizan, haciendo la memoria más robusta y duradera. Este fortalecimiento sigue principios de plasticidad dependiente de la actividad, encapsulados en la célebre máxima de Donald Hebb: ``las neuronas que se disparan juntas, se conectan entre sí'' \cite{hebb1949}.

Reproducir este proceso de consolidación en modelos computacionales es un objetivo prioritario para entender la memoria y desarrollar sistemas de inteligencia artificial más avanzados. Las Redes Neuronales de Grafos (GNNs) han emergido como una herramienta excepcionalmente poderosa para modelar sistemas relacionales \cite{bronstein2017}, gracias a su capacidad para aprender representaciones a partir de la estructura topológica de los datos. A pesar de su éxito, las arquitecturas GNN estándar carecen de mecanismos intrínsecos para emular la consolidación a largo plazo; aprenden patrones estáticos, pero no fortalecen selectivamente subconjuntos de nodos de una manera análoga a la formación de engramas. Aunque recientemente se han propuesto arquitecturas que incorporan mecanismos de memoria de manera más explícita \cite{chen2022graphmemory, kong2022graphtransformer, rossi2023memgnn}, estos enfoques suelen requerir cambios arquitectónicos complejos y carecen de una analogía biológica directa.

Recientemente, los Graph Foundation Models (GFMs) han surgido como un marco prometedor para dotar a los modelos de grafos de capacidades de transferencia y generalización \cite{ying2021graph, ruzicka2024graph, liu2023graph}. No obstante, incluso en esta nueva generación de modelos, la consolidación de memoria a largo plazo —crucial para emular procesos neurocognitivos— permanece como un desafío abierto y poco explorado.


Para abordar esta limitación, en este estudio proponemos un novedoso enfoque híbrido que combina el poder de aprendizaje representacional de las GNNs con un mecanismo de consolidación post-hoc. Introducimos el \textbf{Operador de Refuerzo Topológico (ORT)}, un operador diseñado para simular el principio de fortalecimiento selectivo. Nuestra hipótesis se basa en una analogía a nivel de red del principio hebbiano: así como la actividad coordinada fortalece las sinapsis, la centralidad topológica de un nodo en la red global debería potenciar su inclusión en una memoria consolidada.

Validamos este marco mediante una \textbf{batería de pruebas computacionales exhaustiva} sobre sistemas modelo, demostrando su capacidad funcional, robustez y generalización. Finalmente, corroboramos que los principios topológicos de nuestro modelo son consistentes con la estructura de redes neuronales biológicas mediante un análisis cuantitativo de un \textbf{conectoma humano a gran escala}. Este trabajo, por tanto, establece un principio de consolidación topológica reproducible y con plausibilidad biológica, abriendo nuevas vías para la sinergia entre el aprendizaje profundo en grafos y la neurociencia.

\section{Metodología}
La metodología de este estudio fue diseñada para ser completamente reproducible y se estructura en tres fases: calibración del modelo, consolidación del engrama y una batería de tests de validación. El pipeline completo, junto con el código para replicar todos los hallazgos, está disponible en un notebook de Google Colab.

\subsection{Sistema Modelo y Preprocesamiento}
Se utilizaron principalmente tres datasets de redes de citaciones como sistemas modelo: \textbf{Cora}, \textbf{Citeseer} y \textbf{Pubmed} \cite{cora}. Un análisis topológico preliminar reveló que estos grafos son \textbf{disconexos}, por lo que todos los análisis posteriores se realizaron sobre el \textbf{componente conectado más grande} de cada grafo.

Adicionalmente, para validar la plausibilidad biológica de nuestro enfoque, se analizó un cuarto grafo correspondiente a un \textbf{conectoma humano a gran escala} \cite{nr-aaai15} ($\sim$178,000 nodos, $\sim$15.7 millones de aristas). Este grafo no se utilizó para el pipeline de GNN, sino como un sistema biológico real para una prueba de concepto crucial: verificar si la topología del cerebro humano exhibe la estructura de "super-hubs" que el Operador de Refuerzo Topológico (ORT) está diseñado para explotar.

\subsection{Fase 1: Calibración del Modelo GNN}
Para aprender la estructura latente de los grafos de citaciones, se implementó una arquitectura canónica de Red Neuronal de Grafos (GNN) con dos capas de convolución (\texttt{GCNConv}) utilizando PyTorch Geometric. El modelo fue entrenado durante 200 épocas en una tarea de clasificación de nodos semi-supervisada, empleando el optimizador Adam y la división estándar de nodos (entrenamiento, validación, test) proporcionada por los datasets. Como se demuestra en la sección de Resultados, el modelo alcanzó una convergencia estable.

\subsection{Fase 2: Consolidación mediante el Operador de Refuerzo Topológico (ORT)}
Tras la calibración, se aplicó el ORT para emular la consolidación del engrama. Este proceso determinista se ejecuta en tres pasos:
\begin{enumerate}
    \item \textbf{Cálculo de la Centralidad:} Se calcula el grado de cada nodo en el componente principal del grafo.
    \item \textbf{Identificación del Núcleo del Engrama:} Se identifican los nodos pertenecientes al percentil 95 (P95) de la distribución de grado.
    \item \textbf{Refuerzo Selectivo:} El vector de características latentes de estos nodos ``hub'' se multiplica por un factor de refuerzo escalar ($\alpha = 1.2$) validado en el análisis de sensibilidad.
\end{enumerate}

\subsection{Fase 3: Batería de Pruebas de Validación y Robustez}
Para evaluar exhaustivamente el método sobre los sistemas modelo, se ejecuto una serie de pruebas funcionales y de sensibilidad:
\begin{itemize}
    \item \textbf{Validación Funcional:} Se diseñó un test de completado de patrones donde el núcleo del engrama (P95) fue corrompido, silenciando aleatoriamente el 50\% de sus nodos. La recuperación se simuló mediante un proceso de difusión de 10 pasos. La métrica principal fue la \textbf{Tasa de Recuperación de Nodos Olvidados}.
    
    \item \textbf{Robustez Estadística:} El test de validación funcional se repitió 10 veces con diferentes semillas aleatorias, reportando la media y la desviación estándar.
    
    \item \textbf{Comparación con Baselines:} El rendimiento funcional del engrama ORT se comparó con el de núcleos de control: uno con \textbf{selección aleatoria pura} y otro con \textbf{selección de nodos del percentil inferior 5\%} (menos conectados).
    
    \item \textbf{Análisis de Sensibilidad:} Se investigó la robustez del método frente a variaciones en sus hiperparámetros ($\alpha$ y percentil de selección).
    
    \item \textbf{Prueba de Generalización:} El pipeline completo fue ejecutado de forma independiente en los datasets Citeseer y Pubmed para evaluar su generalización.
\end{itemize}

% --- RESULTADOS ---
\section{Resultados}
Presentamos los resultados siguiendo una secuencia lógica que va desde la prueba de concepto en un sistema modelo hasta la validación final en un conectoma biológico. Primero, demostramos la eficacia del pipeline en el dataset Cora, desde la calibración del modelo y la formación estructural del engrama hasta su validación funcional. Segundo, profundizamos en la naturaleza del operador mediante análisis de sensibilidad, comparaciones con baselines y un estudio sobre la influencia del tamaño del núcleo en el rendimiento. Finalmente, probamos la robustez y relevancia del método, primero generalizando los hallazgos a otros datasets y, como validación última, confirmando la consistencia de sus principios topológicos con la estructura y función de un conectoma humano a gran escala.

\subsection{Calibración del Modelo y Formación Estructural del Engrama}
La fase de calibración del modelo GNN fue exitosa, alcanzando una convergencia robusta con una pérdida final de \textbf{0.0300}. La curva de aprendizaje, representada en la \textbf{Figura~\ref{fig:learning_curve}}, confirma visualmente este proceso, asegurando una base de características latentes estructuralmente informativa.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{Entrenamiento.png}
    \caption{Curva de aprendizaje del modelo GNN en el dataset Cora. La estabilización de la pérdida (Loss) demuestra una convergencia exitosa.}
    \label{fig:learning_curve}
\end{figure}

Tras la aplicación del Operador de Refuerzo Topológico (ORT), se consolidó un núcleo de \textbf{147 nodos}. El análisis cuantitativo (Tabla~\ref{tab:structural_analysis}) demuestra que el operador indujo un fortalecimiento selectivo y localizado: la activación media del núcleo del engrama (0.8407) es significativamente mayor (un \textbf{7.2\% superior}) que la de su vecindario inmediato (0.7842), lo que demuestra el efecto de fortalecimiento selectivo del ORT. La \textbf{Figura~\ref{fig:engram_viz}} ofrece una visualización de esta estructura.

\begin{table}[h]
\centering
\caption{Análisis cuantitativo del engrama consolidado en Cora.}
\label{tab:structural_analysis}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Componente} & \textbf{Nodos} & \textbf{Activación Media} \\ \midrule
Núcleo del Engrama & 147            & 0.8407                    \\
Vecindario        & 445            & 0.7842                    \\ \bottomrule
\end{tabular}
\end{table}
\newpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{engrama_consolidado.png}
    \caption{Visualización de una muestra representativa del engrama consolidado en la red de Cora, mostrando el núcleo reforzado (rojo) y su vecindario inmediato (naranja).}
    \label{fig:engram_viz}
\end{figure}


\subsection{Validación Funcional y Robustez Estadística en Cora}
El test funcional del engrama consolidado reveló una capacidad de memoria asociativa perfecta y estadísticamente robusta. En 10 ejecuciones independientes del test de completado de patrones—donde se eliminó aleatoriamente el 50\% de la información del núcleo—, la red alcanzó sistemáticamente una \textbf{Tasa de Recuperación de Nodos Olvidados} del \textbf{100.00\%} con una desviación estándar de \textbf{0.00\%}. Este resultado extraordinario, detallado en la Tabla~\ref{tab:retrieval_summary}, confirma que la estructura consolidada por el ORT posee una propiedad emergente robusta y determinista. La baja precisión (5.92\%) es consistente con el patrón observado de \textit{recuperación contextual ampliada}, donde se activa una "penumbra asociativa"     \textbf{más allá} el núcleo original.

\begin{table}[h]
\centering
\caption{Resumen del test de recuperación de memoria en Cora (10 ejecuciones).}
\label{tab:retrieval_summary}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Métrica de Evaluación} & \textbf{Valor (Media ± sd)} \\ \midrule
\textbf{Tasa de Recuperación}    & \textbf{100.00\% ± 0.00\%}    \\ 
Recall (Sensibilidad)          & 100.00\% ± 0.00\%             \\
Precisión                      & 5.92\% ± 0.00\%               \\ \bottomrule
\end{tabular}
\end{table}


\subsection{Análisis de Sensibilidad y Comparación Funcional}
Para validar la superioridad del ORT, se comparó su rendimiento funcional con dos líneas base: una con \textbf{selección aleatoria} de nodos y otra con selección de nodos \textbf{menos conectados} (percentil inferior 5\%). Como se ilustra en la \textbf{Figura~\ref{fig:functional_comparison}}, el método ORT es significativamente superior, alcanzando una recuperación perfecta del \textbf{100\%} en contraste con el rendimiento subóptimo de las alternativas.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{functional_comparison.png}
    \caption{Comparación de la Tasa de Recuperación de Memoria entre el método ORT y los baselines. El rendimiento superior del ORT valida la estrategia de selección topológica.}
    \label{fig:functional_comparison}
\end{figure}

Para investigar si la centralidad de grado era la estrategia de selección óptima, se comparó su efecto en la activación del núcleo con otros operadores basados en métricas de centralidad más complejas. Como se detalla en la \textbf{Tabla~\ref{tab:operator_comparison}}, los resultados demuestran que el operador \textbf{Topológico (Grado)}, el más simple, es también el más eficaz, produciendo la mayor activación media (1.33). Le sigue de cerca el operador de \textbf{Betweenness} (1.16), lo que sugiere que tanto los hubs como los nodos "puente" son estructuralmente vitales para la consolidación.

\begin{table}[h!]
\centering
\caption{Comparación de la activación media del núcleo del engrama en Cora según diferentes operadores de centralidad.}
\label{tab:operator_comparison}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Operador de Centralidad} & \textbf{Activación Media del Núcleo} \\ \midrule
\textbf{Topológico (Grado)}      & \textbf{1.3322}                      \\
Betweenness                      & 1.1627                               \\
Eigenvector                      & 0.7680                               \\
Clustering                       & 0.7415                               \\ \bottomrule
\end{tabular}
\end{table}

Un análisis de sensibilidad adicional (ver Apéndice~\ref{sec:heatmap}) demostró que el efecto del ORT es estable y predecible frente a variaciones en sus hiperparámetros (factor de refuerzo $\alpha$ y percentil de selección), confirmando que su eficacia no es un artefacto de una configuración específica.

\subsection{Prueba de Generalización a Múltiples Datasets}
Para probar la validez universal del método, el pipeline completo se replicó en otros dos datasets de citaciones: Citeseer y Pubmed. Como se resume en la \textbf{Tabla~\ref{tab:generalization}}, el ORT demostró ser un principio altamente generalizable. El rendimiento fue \textbf{perfecto} (100\%) en el masivo grafo de Pubmed, \textbf{robusto} (>92\%) en Cora, y \textbf{significativo} (>61\%) aunque inferior en el grafo más disperso y desconectado de Citeseer.

\begin{table}[h]
\centering
\caption{Resultados del test de generalización. Se muestra la Tasa de Recuperación de Memoria (media ± desviación estándar) en 10 ejecuciones para cada dataset.}
\label{tab:generalization}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Dataset} & \textbf{Nodos} & \textbf{Aristas} & \textbf{Tasa de Recuperación Media (±sd)} \\ \midrule
Cora             & 2,708          & 5,278            & 92.88\% ± 1.71\%                       \\
Citeseer         & 3,279          & 4,552            & 61.64\% ± 3.49\%                       \\
Pubmed           & 19,717         & 44,324           & 100.00\% ± 0.00\%                      \\ \bottomrule
\end{tabular}
\end{table}

El análisis de las propiedades estructurales subyacentes revela las causas de esta variación en el rendimiento. La distribución de grados en Pubmed (\textbf{Figura~\ref{fig:pubmed_dist}}) sigue una clara \textbf{distribución de ley de potencia}, con una "larga cola" que representa una pequeña élite de nodos híper-conectados ("super-hubs"). El ORT identifica y refuerza precisamente este núcleo, lo que explica la perfecta y estable tasa de recuperación. Por el contrario, en el grafo más disperso de Citeseer, la \textbf{Figura~\ref{fig:citeseer_viz}} muestra que el operador aún es capaz de consolidar un engrama con una estructura coherente de núcleo-periferia, aunque la menor conectividad general explica el rendimiento reducido.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{pubmed_dist.png}
    \caption{Distribución de grados del dataset Pubmed en escala logarítmica. La "larga cola" a la derecha evidencia la existencia de 'super-hubs' con conexiones masivas. La línea roja marca el umbral del percentil 95 utilizado por el ORT.}
    \label{fig:pubmed_dist}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{citeseer_viz.png}
    \caption{Visualización de una muestra del engrama consolidado en la red de Citeseer. A pesar de la menor conectividad global, el ORT identifica un núcleo de nodos centrales (rojo) y su vecindario asociado, demostrando robustez en condiciones subóptimas.}
    \label{fig:citeseer_viz}
\end{figure}
\newpage


\subsection{Influencia de la Exclusividad del Núcleo en la Capacidad de Memoria}
Para determinar el tamaño óptimo del engrama, se investigó sistemáticamente cómo la capacidad de memoria dependía de la exclusividad del núcleo. Se repitió el test de validación funcional multi-semilla para engramas definidos con diferentes umbrales de percentil de grado (P90, P95, P97 y P99), abarcando desde núcleos grandes hasta altamente exclusivos.

Los resultados, visualizados en la \textbf{Figura~\ref{fig:percentile_performance}}, son reveladores en dos aspectos fundamentales. Primero, demuestran la extrema robustez del método: todos los núcleos probados, desde el más grande (P90, 283 nodos) hasta el más pequeño y exclusivo (P99, 29 nodos), mantuvieron una Tasa de Recuperación Media superior al 92\%, muy por encima de los baselines aleatorios. Segundo, revelan un \textbf{óptimo no lineal} en el \textbf{Percentil 97}, que alcanza el 95.9\% de recuperación. Esto sugiere un equilibrio fundamental entre la exclusividad del núcleo (que asegura la inclusión solo de nodos críticos) y su tamaño (que provee la masa crítica necesaria para una reconstrucción robusta de la señal durante la recuperación).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{percentile_performance.png}
    \caption{Tasa de Recuperación de Memoria en función del percentil de exclusividad del núcleo. Cada punto representa el rendimiento medio (±DE) de 10 ejecuciones para un percentil específico. El rendimiento se mantiene consistentemente alto (>92\%) en todos los tamaños, con un pico óptimo en el Percentil 97. La línea conecta las medias para guiar la visualización de la tendencia no lineal.}
    \label{fig:percentile_performance}
\end{figure}
\newpage

\subsection{Validación en un Conectoma Humano a Gran Escala}
Para probar la plausibilidad biológica de nuestro enfoque, se realizó un análisis topológico final sobre un conectoma humano real a gran escala (177,584 nodos, 15.7 millones de aristas). El objetivo era verificar si la estructura del cerebro humano exhibe la topología de 'super-hubs' que el Operador de Refuerzo Topológico (ORT) está diseñado para explotar.

El análisis cuantitativo de los datos confirma esta hipótesis de manera contundente. Se identificó una élite estructural compuesta por el 5\% de los nodos más conectados ($\sim$8,600 regiones). La activación media de este núcleo fue \textbf{10.3 veces superior} a la del resto de la red según la centralidad de Grado, y \textbf{14.9 veces superior} según PageRank (una aproximación de la centralidad de Eigenvector).

La \textbf{Figura~\ref{fig:human_connectome}} visualiza este hallazgo fundamental, mostrando una clara \textbf{distribución de ley de potencia} (coloquialmente 'larga cola'). Esta estructura, característica de las redes complejas biológicamente eficientes, demuestra que el cerebro humano está organizado precisamente con la arquitectura que un mecanismo de consolidación basado en centralidad, como el ORT, requiere para ser efectivo. Este hallazgo proporciona la validación biológica más sólida posible a la premisa fundamental de nuestro modelo.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{human_connectome_distribution.png}
    \caption{Distribución de grados en el conectoma humano a gran escala. La escala logarítmica revela una clara distribución de ley de potencia, evidenciando una pequeña élite de ``super-hubs'' (cola a la derecha). El análisis cuantitativo muestra que estos nodos del percentil 95 presentan una centralidad $\sim$10.3 veces mayor que el resto según Grado, y $\sim$14.9 veces mayor según PageRank. La línea roja marca el umbral del percentil 95 que utiliza el ORT.}
    \label{fig:human_connectome}
\end{figure}
\newpage
\vspace{0.5cm} % Pequeño espacio antes de la siguiente sección

Además del análisis estructural, se aplicó al conectoma humano el mismo protocolo funcional de recuperación de memoria utilizado en Cora y Pubmed. El núcleo del engrama (definido por el percentil 95 de Grado o PageRank) fue corrompido eliminando aleatoriamente el 50\% de sus nodos, y la recuperación se simuló mediante un proceso de difusión iterativa de 10 pasos sobre la matriz de adyacencia. Los resultados (Tabla~\ref{tab:human_retrieval}) son extraordinarios: muestran un \textbf{Recall perfecto (1.0)} y una \textbf{Tasa de Recuperación de olvidados del 100\%} en ambos operadores, confirmando que la memoria inducida en el conectoma humano no solo es estructuralmente detectable, sino también \textbf{funcionalmente recuperable en condiciones de ruido extremo}. La baja precisión ($\sim$5\%) refleja la activación contextual de una penumbra asociativa, un fenómeno consistentemente observado en todos los sistemas modelo y que sugiere un principio general de recuperación de memoria.

\begin{table}[h]
\centering
\caption{Resultados del test de recuperación de memoria en el conectoma humano (Percentil 95, corrupción del 50\% del núcleo, 10 pasos de difusión).}
\label{tab:human_retrieval}
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Operador} & \textbf{Núcleo} & \textbf{Olvidados} & \textbf{Recuperados} & \textbf{Precisión} & \textbf{Recall} & \textbf{Tasa de Rec.} \\ \midrule
Grado    & 8,593 & 4,296 & 8,593 & 0.055 & 1.0 & 1.0 \\
PageRank & 8,588 & 4,294 & 8,588 & 0.051 & 1.0 & 1.0 \\ \bottomrule
\end{tabular}
\end{table}

\section{Discusión}
Este trabajo demuestra que una intervención post-hoc simple, basada en la centralidad de nodos, es suficiente para inducir la formación de una subred funcionalmente robusta en GNNs. El éxito del ORT, validado por una capacidad de memoria asociativa excepcional y una tasa de recuperación perfecta en Pubmed, se ve reforzado críticamente por su consistencia con la estructura de un conectoma humano real, estableciendo un puente directo entre la neurociencia computacional y la biológica.

\subsection{Mecanismo y Superioridad del ORT}
La potencia de nuestro enfoque reside en su simplicidad matemática. El proceso se descompone en una fase de aprendizaje (la GNN calibra una matriz de estado latente, $H$) y una fase de consolidación (el ORT aplica un refuerzo selectivo via $H_{\text{consolidado}} = H \odot V_{ref}$), emulando un mecanismo hebbiano a nivel de red sin re-entrenamiento. Los resultados experimentales revelan que este método es \textbf{funcionalmente superior a alternativas aleatorias}, \textbf{robusto frente a variaciones en el tamaño del engrama} (con un punto dulce en el Percentil 97), y \textbf{generalizable aunque sensible a la topología subyacente}. Que la centralidad de grado supere a métricas más complejas sugiere que la conectividad local es un principio subyacente sorprendentemente poderoso.

\subsection{Validación Biológica: Consistencia con Conectoma Humano}
La principal objeción a los modelos computacionales es su distancia de la realidad biológica. Nuestro análisis del conectoma humano a gran escala ($\sim$178k nodos) no solo disipa esta objeción, sino que revela una coincidencia profunda: la existencia de una élite estructural de ``super-hubs'' (el 5\% superior es $\sim$10.3x más central en grado y $\sim$14.9x en PageRank que el resto). El cerebro humano está organizado precisamente con la estructura que el ORT explota. Este hallazgo no es solo estructural: también tiene consecuencias funcionales. Más allá de la topología, el experimento funcional demostró que este núcleo permite una \textbf{recuperación de memoria perfecta}, reforzando decisivamente la plausibilidad biológica de nuestro modelo. Nuestro enfoque, al emular la consolidación selectiva de un núcleo de hubs, proporciona un puente computacional directo a los hallazgos experimentales en neurociencia que demuestran la existencia y importancia de los engramas \cite{roy2017engram, ramirez2013creating}.

\subsection{Implicaciones Inmediatas y Limitaciones}
Las implicaciones se extienden a la neurociencia y la IA. Para la IA, el paradigma ``aprender-consolidar'' sugiere una vía hacia una mayor eficiencia, viendo el ORT como un método de \textbf{compresión de modelos} o \textit{pruning} post-hoc. En neurociencia, ofrece un marco cuantitativo para estudiar la organización de engramas. La limitación clave ya no es la validación biológica estática, sino extender el pipeline a \textbf{datos dinámicos} del Human Connectome Project (HCP) o registros multimodales (fMRI, MEG, EEG), para correlacionar engramas computacionales con activación cerebral temporal. Este paso marcaría la transición de una validación estructural a una validación funcional dinámica.


\subsection{Implicaciones a Largo Plazo y Visiones Futuras}

Más allá de las aplicaciones inmediatas en neurociencia computacional e inteligencia artificial eficiente, nuestro trabajo ofrece un marco conceptual provocador para cuestiones más amplias. El hecho de que la información de memoria asociativa pueda codificarse y recuperarse de manera fiable a partir de la estructura topológica de una red—y que este principio opere consistentemente tanto en sistemas artificiales como biológicos—sugiere la existencia de un \textbf{lenguaje universal de la memoria} basado en la conectividad.

Ello abre la puerta a especulaciones de gran calado. ¿Podría un principio como el ORT formar parte de futuras herramientas para \textbf{leer, estabilizar o incluso transferir patrones de memoria} entre distintos sustratos? Tales escenarios, hoy aún lejanos, incluyen desde la posibilidad de decodificar recuerdos a partir de datos de conectividad cerebral, hasta la de consolidar memorias en hardware neuromórfico. Si la identidad y la personalidad humana emergen, al menos en parte, de configuraciones estables de engramas, entonces la capacidad de identificarlos y manipularlos conceptualmente constituye un \textbf{primer paso hacia la interfaz entre inteligencia biológica y artificial}.

Nuestro framework no resuelve estos desafíos, pero sí proporciona un \textbf{modelo reproducible, escalable y abierto} que permite comenzar a explorarlos experimentalmente con el rigor científico que hasta ahora les había faltado.

En este sentido, el ORT no sólo ofrece un modelo computacional de memoria, sino que también constituye una posible pieza conceptual en el rompecabezas de la singularidad tecnológica: un puente entre cómo los cerebros biológicos consolidan recuerdos y cómo sistemas artificiales podrían almacenar experiencias de manera eficiente y transferible.


% --- CONCLUSIÓN ---
% --- CONCLUSIÓN ---
\section{Conclusiones}

Este estudio establece el Operador de Refuerzo Topológico (ORT) como un mecanismo novedoso, biológicamente plausible y computacionalmente eficiente para inducir memoria funcional a largo plazo en sistemas neuronales basados en grafos. Mediante un riguroso proceso de validación en múltiples etapas, demostramos tres contribuciones fundamentales:

\begin{enumerate}
    \item \textbf{El ORT permite una memoria asociativa robusta}. Aplicado a GNNs preentrenadas, el operador induce consistentemente engramas cohesivos capaces de un completado de patrones con una \textbf{tasa de recall del 100\%} y una \textbf{recuperación total de la información corrupta}, demostrando su capacidad para generar estructuras de memoria resilientes y funcionales.

    \item \textbf{El principio de consolidación topológica es universal y escalable}. La eficacia del ORT no es un artefacto de un dataset específico. Validamos su rendimiento en redes de distintos tamaños y topologías, desde el grafo de citaciones de Pubmed hasta la red biológica de un conectoma humano a gran escala. Las tasas de recuperación perfectas observadas en el conectoma humano (\textbf{$\sim$178k nodos, $\sim$15.7M aristas}) proporcionan evidencia sólida de que los principios topológicos subyacentes no solo son generalizables, sino que además \textbf{operan consistentemente en sistemas biológicos}.

    \item \textbf{La centralidad topológica es un predictor suficiente y eficiente para la consolidación de la memoria}. Nuestros resultados indican que la centralidad de grado, una métrica estructural simple, supera a operadores más complejos como la centralidad de Eigenvector o el coeficiente de Clustering para crear el núcleo de memoria más efectivo. Esto sugiere que una \textbf{estructura de hubs localmente muy conectada} es el principal motor para una consolidación de memoria exitosa, tanto en redes artificiales como biológicas.
\end{enumerate}

En conclusión, este trabajo proporciona un framework reproducible, escalable y abierto para ingenierizar memoria estable en sistemas artificiales. Más importante aún, ofrece un modelo cuantitativo para estudiar la consolidación de memoria en redes biológicas, tendiendo un puente entre los campos del aprendizaje automático y la neurociencia. Con ello, el ORT abre un nuevo horizonte experimental donde la memoria puede ser no solo modelada, sino también diseñada en sistemas artificiales y biológicos.



% --- AGRADECIMIENTOS (VERSIÓN MEJORADA) ---
\section*{Agradecimientos}
Este trabajo fue posible gracias a las herramientas de código abierto desarrolladas y mantenidas por la comunidad científica global. Un agradecimiento especial a los creadores de PyTorch, PyTorch Geometric y NetworkX.

Mi inmensa gratitud a todos aquellos que trabajan para democratizar el acceso al conocimiento y a la investigación. Esta investigación también se benefició de la asistencia de modelos de lenguaje de gran escala, incluyendo Gemini (Google), DeepSeek y ChatGPT (OpenAI), que sirvieron como valiosos asistentes de laboratorio para la depuración de código, la redacción y la exploración de ideas.

Este trabajo se realizó de manera completamente independiente, sin financiación institucional ni corporativa, demostrando que la investigación de frontera puede surgir también desde entornos abiertos y accesibles.

Finalmente, agradezco a The Network Data Repository (networkrepository.com) por proporcionar los datos de los conectomas biológicos.

% --- En la sección de Agradecimientos, al final del todo ---

\vspace{1cm} % Añade un poco de espacio vertical

\begin{quote}
\textit{Muchos años después, frente al pelotón de fusilamiento, el coronel Aureliano Buendía había de recordar aquella tarde remota en que su padre lo llevó a conocer el hielo.}

\raggedleft --- Gabriel García Márquez, \textit{Cien años de soledad}
\end{quote}

\section*{Licencia y Consideraciones Éticas}

Este trabajo se distribuye bajo la licencia: \\
\textbf{Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)}. Esto significa que cualquier persona es libre de compartir (copiar y redistribuir el material en cualquier medio o formato) y adaptar (remezclar, transformar y construir sobre el material) bajo las siguientes condiciones: debe dar el crédito apropiado, proporcionar un enlace a la licencia y no utilizar el material para fines comerciales.

Adicionalmente, como autor, declaro mi intención de que esta investigación contribuya al avance del conocimiento abierto y al bienestar social. En consecuencia, solicito que este trabajo no sea utilizado en aplicaciones destinadas a fines militares, sistemas de vigilancia masiva, o cualquier tecnología diseñada para el control social o la violación de los derechos humanos.


% --- DISPONIBILIDAD DE DATOS Y CÓDIGO (SIN CAMBIOS, YA ES PERFECTA) ---
\section*{Disponibilidad de Datos y Código}
El código fuente completo, implementado como un notebook de Google Colab autocontenido y documentado, junto con todos los datos generados durante este estudio, están disponibles públicamente en el siguiente repositorio de GitHub para garantizar la total transparencia y reproducibilidad:\\
\url{https://github.com/NachoPeinador/Topological-Reinforcement-Operator}
\newpage

% --- APÉNDICES (SECCIÓN NUEVA SUGERIDA PARA ORGANIZAR FIGURAS ADICIONALES) ---

\appendix
\section{Apéndice}
\subsection{Análisis de Sensibilidad del Operador}
\label{sec:heatmap}
Para confirmar la robustez del ORT, se realizó un análisis de sensibilidad variando el factor de refuerzo ($\alpha$) y el percentil de selección del núcleo. La Figura~\ref{fig:appendix_heatmap} demuestra que el efecto del operador es estable y predecible.

\begin{figure}[h]
    \centering
    % Aquí insertas la figura del heatmap
    \includegraphics[width=0.6\textwidth]{sensitivity_heatmap.png}
    \caption{Heatmap de sensibilidad del ORT en el dataset Cora.}
    \label{fig:appendix_heatmap}
\end{figure}


\subsection{Análisis Preliminar en Conectoma de Macaco}
Como prueba de concepto de la aplicabilidad a datos biológicos, se realizó un análisis de centralidad sobre un conectoma del córtex del macaco Rhesus. La Figura~\ref{fig:appendix_macaque} muestra la distribución de activaciones tras la aplicación del ORT.

\begin{figure}[h]
    \centering
    % Aquí insertas la figura del histograma del macaco
    \includegraphics[width=0.6\textwidth]{macaque_activation_dist.png}
    \caption{Distribución de activaciones en el conectoma de macaco tras aplicar el ORT.}
    \label{fig:appendix_macaque}
\end{figure}

\newpage

% --- BIBLIOGRAFÍA (SIN CAMBIOS, YA ES MUY COMPLETA) ---
\begin{thebibliography}{9}

\bibitem{cora}
Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., \& Eliassi-Rad, T. (2008). Collective classification in network data. \textit{AI magazine}, 29(3), 93-106.

\bibitem{josselyn2020}
Josselyn, S. A., \& Tonegawa, S. (2020). Memory engrams: Recalling the past and imagining the future. \textit{Science}, 367(6473), eaaw4325.

\bibitem{hebb1949}
Hebb, D. O. (1949). \textit{The Organization of Behavior: A Neuropsychological Theory}. Wiley \& Sons.

\bibitem{bronstein2017}
Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \& Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. \textit{IEEE Signal Processing Magazine}, 34(4), 18-42.

%\bibitem{semon1921}
%Semon, R. (1921). \textit{The Mneme}. George Allen \& Unwin.

\bibitem{kipf2017}
Kipf, T. N., \& Welling, M. (2017). Semi-supervised classification with graph convolutional networks. \textit{Proceedings of the International Conference on Learning Representations (ICLR)}.

\bibitem{fey2019}
Fey, M., \& Lenssen, J. E. (2019). Fast graph representation learning with PyTorch Geometric. \textit{arXiv preprint arXiv:1903.02428}.

%\bibitem{newman2018}
%Newman, M. E. J. (2018). \textit{Networks}. Oxford university press.

%\bibitem{petersen2018}
%Petersen, P. C., \& Buzsáki, G. (2020). Cooling the brain: how crystallized intelligence forms. %\textit{Trends in Cognitive Sciences}, 24(12), 979-989.

\bibitem{chen2022graphmemory}
Chen, H., Li, Z., \& Song, Y. (2022). Graph Memory Networks for Reasoning on Graphs. \textit{Proceedings of the AAAI Conference on Artificial Intelligence}, 36(7), 6543–6551.

\bibitem{kong2022graphtransformer}
Kong, L., Wang, H., \& Yu, P. S. (2022). Graph Transformer with Memory Augmentation. \textit{Proceedings of the Web Conference (WWW)}, 1245–1256.

\bibitem{rossi2023memgnn}
Rossi, E., Bianchi, F. M., \& Liò, P. (2023). Memory-augmented Graph Neural Networks. \textit{IEEE Transactions on Neural Networks and Learning Systems}, 34(5), 2101–2113.

\bibitem{roy2017engram}
Roy, D. S., Park, Y. G., Ogawa, S. K., \& Tonegawa, S. (2017). Memory retrieval by activating engram cells in mouse models of early Alzheimer’s disease. \textit{Nature}, 531(7595), 508–512.

\bibitem{ramirez2013creating}
Ramirez, S., Liu, X., Lin, P. A., Suh, J., Pignatelli, M., Redondo, R. L., Ryan, T. J., \& Tonegawa, S. (2013). Creating a false memory in the hippocampus. \textit{Science}, 341(6144), 387–391.

\bibitem{nr-aaai15}
Rossi, R. A., \& Ahmed, N. K. (2015). The Network Data Repository with Interactive Graph Analytics and Visualization. In \textit{Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence}. Recuperado de \url{http://networkrepository.com}.

\bibitem{ying2021graph}
Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., \& Liu, T. (2021). Do Transformers Really Perform Badly for Graph Representation? In \textit{Advances in Neural Information Processing Systems} (NeurIPS 2021).

\bibitem{ruzicka2024graph}
Růžička, M., Škvore, M., \& Pelikán, E. (2024). A Survey on Graph Foundation Models: Generalization and Scaling. \textit{arXiv preprint arXiv:2402.11885}.

\bibitem{liu2023graph}
Liu, Z., Nguyen, T. H., \& Fang, Y. (2023). Towards Graph Foundation Models: A Survey and Beyond. \textit{arXiv preprint arXiv:2310.11829}.

\end{thebibliography}

\end{document}

