\documentclass[12pt, a4paper]{article}

% --- PAQUETES ---
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\usepackage{booktabs} % Para tablas de alta calidad
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{caption}

% --- DEFINICIÓN DE COLORES Y ESTILOS ---
\definecolor{oxfordblue}{RGB}{0, 33, 71}
\hypersetup{
    colorlinks=true,
    linkcolor=oxfordblue,
    urlcolor=oxfordblue,
}
\titleformat{\section}{\large\bfseries\color{oxfordblue}}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}

% --- METADATOS DEL DOCUMENTO ---
\title{El Operador de Refuerzo Topológico (ORT): Un Principio de Parsimonia para la Consolidación de Memoria en Redes Complejas}

\author{José Ignacio Peinador Sala \\ \textit{Investigador Independiente} \\ \href{https://orcid.org/0009-0008-1822-3452}{ORCID: 0009-0008-1822-3452}}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
\noindent La consolidación de engramas de memoria es un reto central en neurociencia computacional. Este trabajo presenta el \textbf{Operador de Refuerzo Topológico (ORT)}, un mecanismo post-entrenamiento que refuerza nodos topológicamente relevantes para inducir engramas funcionales en redes complejas. 

Validamos el ORT mediante un protocolo funcional basado en difusión normalizada y F1-score, aplicado a redes de citaciones (Cora, Citeseer, Pubmed) y conectomas biológicos (macaco, humano). Los resultados revelan un principio dual de consolidación: en redes de información, la resiliencia de la memoria emerge de núcleos amplios de \textbf{masa crítica} (P90), mientras que en redes biológicas optimizadas predomina un núcleo de \textbf{élite} más pequeño (P95), con un rendimiento de hasta el 87.4\% en el conectoma humano. 

Finalmente, demostramos que el ORT, basado en centralidad de grado, es $\sim$96 veces más rápido que PageRank, estableciendo un \textbf{principio de parsimonia computacional} que vincula estructura, función y eficiencia en redes neuronales.
\end{abstract}

\section{Introducción}
La memoria posee un sustrato físico en el cerebro conocido como ``engrama'': un conjunto de neuronas que se activan de forma coordinada para codificar una experiencia \cite{josselyn2020}. Para persistir, un engrama debe someterse a un proceso de \textbf{consolidación}, durante el cual las conexiones sinápticas entre sus neuronas se fortalecen y estabilizan \cite{hebb1949}. Este fenómeno no ocurre en el vacío, sino que depende de la compleja arquitectura de conexiones del cerebro, una red optimizada para procesar y almacenar información de manera eficiente \cite{sporns2010}.

Reproducir este proceso en modelos computacionales es un objetivo prioritario para la neurociencia y la inteligencia artificial. Las Redes Neuronales de Grafos (GNNs) son una herramienta poderosa para modelar sistemas relacionales \cite{bronstein2017}, habiéndose consolidado como el estado del arte en una vasta gama de aplicaciones \cite{wu2020}. Sin embargo, carecen de mecanismos intrínsecos para emular la consolidación a largo plazo. Para abordar esta limitación, en este estudio proponemos y validamos rigurosamente el \textbf{Operador de Refuerzo Topológico (ORT)}, un mecanismo post-entrenamiento que simula el principio de fortalecimiento selectivo.

Nuestra hipótesis inicial, basada en una analogía hebbiana a nivel de red, postula que la centralidad topológica de un nodo debería potenciar su inclusión en una memoria consolidada. Sin embargo, un análisis crítico de nuestra metodología preliminar reveló que, si bien la idea era prometedora, su validación funcional presentaba artefactos metodológicos que impedían extraer conclusiones robustas. Este trabajo, por tanto, sigue un itinerario de investigación que parte de esa revisión crítica para construir un marco experimental mucho más sólido. A través de este proceso, refinamos nuestras preguntas científicas para explorar cuestiones más profundas:
\begin{itemize}
    \item \textbf{¿Masa Crítica vs. Élite?:} ¿La resiliencia de la memoria funcional depende de un núcleo grande y redundante de nodos importantes (``masa crítica''), o de un grupo pequeño y altamente selecto (``élite'')?
    \item \textbf{¿Universalidad vs. Especificidad de Dominio?:} ¿El tamaño óptimo del engrama es un principio universal, o varía entre redes de información (como las de citaciones académicas) y redes biológicas (como los conectomas cerebrales), cuya estructura de núcleo ha sido previamente caracterizada \cite{hagmann2008}?
    \item \textbf{¿Especificidad Topológica?:} ¿La eficacia del engrama se debe solo a la importancia individual de sus nodos, o a la organización estructural específica en la que están conectados?
\end{itemize}

Para responder a estas preguntas, validamos el ORT mediante una batería de pruebas computacionales exhaustivas, primero en sistemas modelo (Cora, Citeseer, Pubmed) y finalmente en conectomas biológicos reales (macaco y humano). Este estudio no solo establece un principio de consolidación topológica reproducible y con plausibilidad biológica, sino que también revela una notable diferencia en las estrategias de optimización de memoria entre sistemas artificiales y evolutivos.

\section{Metodología}
La metodología de este estudio se diseñó como una respuesta directa a un análisis crítico de un pipeline experimental preliminar. El enfoque, por tanto, no se limitó a aplicar un método, sino a construir un marco de validación robusto, reproducible y estadísticamente riguroso. Todo el código y los datos para replicar los hallazgos están disponibles en los notebooks de Google Colab complementarios.

\subsection{Diseño Experimental y Plataformas de Prueba}
El estudio se estructuró en dos grandes fases: 1) el desarrollo y validación de la metodología en redes de citaciones académicas, y 2) la aplicación de esta metodología validada a conectomas biológicos para probar su plausibilidad. Se utilizaron los siguientes datasets:
\begin{itemize}
    \item \textbf{Redes de Citaciones:} Se emplearon los grafos estándar \textbf{Cora}, \textbf{Citeseer} y \textbf{Pubmed} \cite{sen2008collective}. Estas redes son ejemplos clásicos de redes libres de escala (\textit{scale-free networks}), caracterizadas por una distribución de grado de ley de potencia donde la mayoría de los nodos tienen pocas conexiones y una minoría de hubs concentra una gran proporción de las aristas \cite{barabasi1999}. Un análisis topológico preliminar reveló que estos grafos son disconexos, por lo que todos los análisis posteriores se realizaron sobre su componente conectado más grande.
    \item \textbf{Conectomas Biológicos:} Para validar la plausibilidad biológica, se analizaron dos conectomas reales: uno de \textbf{macaco Rhesus} (91 nodos, 1,401 aristas) y un conectoma humano a gran escala (\textbf{BNU-1}, $\sim$177,000 nodos, $\sim$15.6 millones de aristas) \cite{nr-aaai15}.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{Scale-Free.png}
    \caption{Distribución de grado en redes representativas. La figura muestra los histogramas de distribución de grado para el dataset Pubmed y el conectoma humano (BNU-1). Ambos, con escala logarítmica en el eje Y, exhiben una característica ''cola larga'', indicativa de una estructura de red libre de escala (\textit{scale-free}). Esta topología, dominada por una minoría de hubs (nodos a la derecha de la línea roja del umbral P95), es el sustrato estructural que el ORT explota.}
    \label{fig:scale_free}
\end{figure}

\subsection{Análisis Crítico y Refinamiento Metodológico}
La metodología original, aunque conceptualmente coherente, presentaba debilidades que inflaban artificialmente las métricas de rendimiento. Las críticas clave y nuestras soluciones metodológicas fueron:
\begin{enumerate}
    \item \textbf{Crítica: Refuerzo No Operativo.} En el test original, el refuerzo del ORT ($\alpha=1.2$) no participaba en la dinámica de recuperación. \textbf{Solución:} Se implementó un mecanismo de difusión estilo PageRank Personalizado donde el vector semilla se pondera por el factor de refuerzo, haciendo que el ORT participe activamente en la recuperación funcional.
    \item \textbf{Crítica: Difusión Saturante y Umbral Sesgado.} La difusión original, basada en la multiplicación de la matriz de adyacencia con recorte a [0,1], saturaba las señales y, combinada con un umbral de percentil, garantizaba un recall cercano al 100\% de forma artificial. \textbf{Solución:} Se sustituyó por una difusión con matriz de adyacencia normalizada y un factor de \textit{damping}, y se reemplazó el umbral por una selección \textit{top-k} objetiva, evaluando el rendimiento con el F1-score.
    \item \textbf{Crítica: Baselines Incompletos y Falta de Rigor.} La comparación original carecía de controles clave (como un baseline estratificado por grado) y de pruebas de significancia estadística. \textbf{Solución:} Se introdujeron múltiples baselines (aleatorio puro, estratificado por grado) y un control topológico mediante \textit{rewiring} que preserva el grado. Todas las comparaciones clave se validaron con tests de significancia no paramétricos (Wilcoxon/Mann-Whitney).
\end{enumerate}

\paragraph{Notas terminológicas.}
Para favorecer la comprensión interdisciplinaria, aclaramos dos conceptos clave:
\begin{itemize}
    \item \textbf{Difusión normalizada:} proceso iterativo que propaga una señal a lo largo del grafo utilizando una matriz de adyacencia reescalada por el grado de cada nodo, evitando la saturación de los valores.
    \item \textbf{Rewiring (recableado):} técnica de control que reconfigura aleatoriamente las conexiones de la red preservando el grado de cada nodo, utilizada para disociar los efectos estructurales globales de la topología específica.
\end{itemize}

\subsection{Protocolo de Validación Funcional Robusta}
A partir del refinamiento anterior, se estableció el siguiente protocolo para todos los tests de memoria asociativa:
\begin{enumerate}
    \item \textbf{Consolidación del Engrama:} Se identifica un núcleo de engrama seleccionando los nodos que superan un percentil de centralidad (P90 o P95).
    \item \textbf{Simulación del Olvido:} Se crea un patrón de memoria dañado silenciando aleatoriamente el 50\% de los nodos del núcleo.
    \item \textbf{Recuperación por Difusión:} Se inicia un proceso de recuperación mediante un algoritmo de difusión de 20 pasos, inspirado en PageRank Personalizado:
    $$ \mathbf{x}_{t+1} = (1 - d) \cdot \mathbf{s}_{\alpha} + d \cdot (\mathbf{\hat{A}} \mathbf{x}_t) $$
    donde $\mathbf{s}_{\alpha}$ es el vector semilla reforzado, $d$ es el factor de \textit{damping} (0.85), y $\mathbf{\hat{A}}$ es la matriz de adyacencia normalizada simétricamente.
    \item \textbf{Evaluación de Rendimiento:} Se identifican los \textit{k} nodos con mayor activación final, donde \textit{k} es el tamaño del núcleo original. El éxito de la recuperación se cuantifica mediante el \textbf{F1-score}. Para asegurar la robustez estadística, este proceso se repite 20-30 veces por condición, reportando la media y la desviación estándar.
\end{enumerate}
\newpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{Sensibilidad.png}
    \caption{Análisis de sensibilidad del Operador de Refuerzo Topológico. El heatmap muestra la ''Activación Media'' del núcleo del engrama en función del factor de refuerzo ($\alpha$, eje Y) y el percentil de selección (exclusividad, eje X). Se observa una respuesta estable y monotónica: la activación aumenta de forma predecible con una mayor exclusividad y un refuerzo más intenso, lo que valida la robustez y el comportamiento controlable del método.}
    \label{fig:sensibilidad}
\end{figure}


\section{Resultados}
Los resultados de este estudio se presentan siguiendo el itinerario de la revisión crítica. Primero, se exponen los hallazgos en las redes de citaciones que demuestran la superación de los artefactos metodológicos y revelan nuevos principios sobre la estructura óptima del engrama. Segundo, se presentan los resultados en conectomas biológicos que validan la plausibilidad y eficiencia del ORT.

\subsection{Revisión Metodológica: Del Recuerdo Perfecto al Rendimiento Robusto}
El análisis crítico de nuestro experimento preliminar identificó correctamente que la métrica de ''recuperación perfecta '' (Recall del 100\%) era un artefacto del método de difusión y del umbral de evaluación sesgado. Al aplicar el protocolo de validación funcional corregido, basado en difusión normalizada y selección \textit{top-k}, obtuvimos una medida mucho más realista del rendimiento. En la red Cora, por ejemplo, la tasa media de recuperación de nodos olvidados se situó en un \textbf{49.4\% $\pm$ 4.7\%}, un resultado sólido pero alejado del 100\% artificial anterior. Para verificar la robustez interna del protocolo, se realizaron pruebas de validación en esta misma red, donde el método demostró una recuperación perfecta (F1-score = 1.0) del núcleo P95, confirmando la validez del mecanismo de difusión y evaluación antes de su aplicación a análisis más complejos.


Como resume la Tabla \ref{table:percentile_comparison_f1}, el núcleo más grande (P90) obtiene los mejores F1-scores en las tres redes. En contraste, al aumentar la exclusividad (P95–P99), el rendimiento decrece progresivamente. Este patrón respalda la hipótesis de \textbf{masa crítica}, donde la redundancia y amplitud del núcleo superan la selección extrema de unos pocos nodos.


\subsection{El Principio de ''Masa Crítica'' vs. ''Élite'' en Redes de Información}
Una de las preguntas centrales de este estudio era determinar el tamaño óptimo del engrama. La hipótesis inicial favorecía un núcleo de  ''élite'' (P95, el 5\% superior de hubs), pero los resultados de nuestros análisis de sensibilidad funcional revelaron un principio universal y consistente en las tres redes de citaciones: \textbf{la eficacia de la memoria es inversamente proporcional a la exclusividad del núcleo}.

Como se muestra en la Tabla \ref{table:percentile_comparison_f1}, el engrama más grande y menos exclusivo (\textbf{P90}, el 10\% superior) obtiene consistentemente el F1-score más alto en Cora (65.8\%), Citeseer (78.2\%) y Pubmed (73.9\%). A medida que el núcleo se hace más pequeño y selecto (P95, P97, P99), el rendimiento funcional se degrada progresivamente. Este hallazgo valida la hipótesis de la \textbf{ ''masa crítica''}: para la memoria asociativa en estas redes, la redundancia y la fuerza colectiva de un engrama más grande son funcionalmente superiores a la importancia individual extrema de un núcleo de élite.

% --- Placeholder para la Tabla 1 ---
\begin{table}[h!]
\centering
\caption{Comparación de F1-Score por Percentil en Redes de Citaciones.}
\label{table:percentile_comparison_f1}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Percentil} & \textbf{Cora (F1-Score)} & \textbf{Citeseer (F1-Score)} & \textbf{Pubmed (F1-Score)} \\ \midrule
P90 & \textbf{0.658 $\pm$ 0.014} & \textbf{0.782 $\pm$ 0.016} & \textbf{0.739 $\pm$ 0.009} \\
P95 & 0.628 $\pm$ 0.025 & 0.744 $\pm$ 0.023 & 0.692 $\pm$ 0.012 \\
P97 & 0.582 $\pm$ 0.029 & 0.723 $\pm$ 0.025 & 0.657 $\pm$ 0.019 \\
P99 & 0.576 $\pm$ 0.041 & 0.671 $\pm$ 0.050 & 0.682 $\pm$ 0.030 \\ \bottomrule
\end{tabular}
\end{table}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{masa_critica.png}
    \caption{Rendimiento funcional (F1-Score) vs. Exclusividad del núcleo en redes de información. Los gráficos de barras comparan el F1-Score promedio ($\pm$ desviación estándar) para núcleos de engrama de tamaño decreciente (P90 a P99) en los datasets Cora, Citeseer y Pubmed. Se observa un patrón universal donde el núcleo más grande e inclusivo (P90), representativo de la ''masa crítica'', obtiene consistentemente el mayor rendimiento funcional.}
    \label{fig:masa_critica}
\end{figure}

\subsection{Superioridad Funcional del ORT: Un Análisis Contexto-Dependiente}
Para validar la ''superioridad funcional concluyente'' del ORT, se realizó una comparación rigurosa contra baselines de control, incluyendo un baseline aleatorio puro y, crucialmente, un \textbf{baseline estratificado} (selección de hubs al azar). Los resultados, resumidos en la Tabla \ref{table:baseline_comparison}, revelaron una dinámica fascinante y dependiente del contexto.

Para el núcleo de ''élite'' \textbf{P95}, la estrategia de selección específica del ORT es drásticamente superior a la selección de hubs al azar, con una tasa de recuperación del 41.96\% frente al 20.05\% del baseline estratificado (p $<$ 0.001). Esto confirma que para engramas pequeños, la selección precisa de los ''super-hubs'' es fundamental.

Sin embargo, para el núcleo de ''masa crítica'' \textbf{P90}, la ventaja del ORT (49.42\%) sobre el baseline estratificado (47.69\%) **no fue estadísticamente significativa** (p = 0.185). Este hallazgo sugiere que, una vez alcanzado un tamaño de núcleo suficiente, el beneficio funcional proviene de la importancia colectiva del grupo de hubs, y no tanto de la selección específica dentro de él.

% --- Placeholder para la Tabla 2 ---
\begin{table}[h!]
\centering
\caption{Comparación Funcional del ORT vs. Baselines en Cora (Tasa de Recuperación).}
\label{table:baseline_comparison}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Método} & \textbf{Nodos en Núcleo} & \textbf{Recuperación Media (\%)} \\ \midrule
\textbf{ORT (P90)} & \textbf{286} & \textbf{49.42 $\pm$ 4.70} \\
Baseline Estratificado (P90) & 286 & 47.69 $\pm$ 5.09 \\
\textbf{ORT (P95)} & \textbf{147} & \textbf{41.96 $\pm$ 5.09} \\
Baseline Estratificado (P95) & 147 & 20.05 $\pm$ 4.15 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Plausibilidad Biológica: El Principio del ''Engrama de Élite'' en Conectomas}
Una de las críticas centrales al estudio preliminar era que la plausibilidad biológica se limitaba al análisis estructural. Para abordar esta cuestión, primero visualizamos la estructura del engrama P95 identificado por el ORT en el conectoma humano, cuya organización núcleo-periferia se muestra en la Figura \ref{fig:engrama_humano}. A continuación, aplicamos nuestro protocolo de validación funcional robusto a dos conectomas biológicos reales: el de un macaco Rhesus y un conectoma humano a gran escala. Los resultados no solo confirmaron la plausibilidad funcional, sino que revelaron una estrategia de consolidación de memoria diferente a la observada en las redes de información.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{engrama_humano.png}
    \caption{Visualización representativa del engrama P95 en el conectoma humano (BNU-1). 
Los nodos del núcleo (rojo), correspondientes al 5\% de los hubs con mayor grado, forman un clúster denso característico del \textit{rich club}. El vecindario (azul claro) representa la periferia asociativa conectada a este núcleo, ilustrando la organización \textit{core–periphery} identificada por el ORT. Por motivos de legibilidad y eficiencia computacional, se muestra un subgrafo representativo del núcleo y su vecindario; el procedimiento completo de generación y parámetros técnicos se documentan en los Notebooks complementarios disponibles en el repositorio asociado al artículo.}
    \label{fig:engrama_humano}
\end{figure}

A diferencia del principio de ''masa crítica'' (donde el núcleo P90 era superior), en el conectoma del macaco, el núcleo más pequeño y exclusivo de \textbf{''élite'' (P95)} demostró un rendimiento funcional óptimo, alcanzando una \textbf{recuperación de memoria perfecta (F1-score = 1.00)} con los operadores que miden la influencia global (Degree, PageRank y Eigenvector).

Este hallazgo se vio reforzado en el conectoma humano a gran escala. Como se muestra en la Tabla \ref{table:human_connectome_results}, el núcleo P95, identificado tanto por Grado como por PageRank, logró una capacidad de memoria asociativa excepcional, con un F1-score medio de hasta el \textbf{87.4\%}. Aunque PageRank demostró ser marginalmente superior al Grado, la diferencia no fue sustancial, mientras que ambos superaron de forma masiva a los controles aleatorios. Este resultado valida de forma contundente que la estructura de hubs del cerebro humano es funcionalmente capaz de sostener un engrama de memoria robusto.

% --- Placeholder para la Tabla 3 ---
\begin{table}[h!]
\centering
\caption{Rendimiento Funcional del ORT (P95) en el Conectoma Humano.}
\label{table:human_connectome_results}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Operador} & \textbf{F1-Score Medio} \\ \midrule
\textbf{ORT (PageRank, P95)} & \textbf{0.874 $\pm$ 0.004} \\
ORT (Degree, P95) & 0.865 $\pm$ 0.005 \\
Baseline Aleatorio & 0.513 $\pm$ 0.001 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Especificidad Topológica y Parsimonia Computacional}
Para probar si la eficacia del engrama se debe a la \textbf{organización topológica específica} del cerebro, se realizó un experimento de control con \textit{rewiring}, una técnica estándar en neurociencia de redes para disociar las propiedades de grado de la topología específica de la red \cite{mahadevan2022}. Los resultados fueron inequívocos y reveladores: el proceso de \textit{rewiring} falló sistemáticamente en los subgrafos del conectoma humano. Como se muestra en la Figura \ref{fig:especificidad_final}, esto provocó un \textbf{colapso total de la función de memoria}, con un F1-score que cayó a cero en todas las réplicas. Este hallazgo es significativamente más fuerte que una simple degradación del rendimiento: sugiere que la arquitectura del conectoma es tan altamente no-aleatoria y optimizada que no puede ser alterada arbitrariamente sin una pérdida catastrófica de su capacidad funcional, confirmando que la memoria es una propiedad emergente de la precisa configuración de la red cerebral.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{especificidad_topologica_final.png}
    \caption{Colapso funcional por alteración topológica en el conectoma humano. El boxplot compara el rendimiento (F1-Score) del ORT (Degree y PageRank) en la topología original frente a un control aleatorio (Random) y redes con topología alterada (\textit{rewiring}). Se observa un colapso total de la función (F1-score $\approx 0$) en las redes recableadas, un resultado estadísticamente muy significativo (p $<$ 0.001). Este fallo sistemático demuestra que la arquitectura del conectoma está tan optimizada que no puede ser alterada sin una pérdida completa de su capacidad funcional para la memoria.}
    \label{fig:especificidad_final}
\end{figure}

Finalmente, para validar la eficiencia del ORT, se realizó un \textit{benchmark} computacional, enmarcado en el conocido desafío de optimizar el cálculo de métricas de centralidad en redes a gran escala \cite{brandes2001}. Se comparó el rendimiento del ORT (basado en Grado) frente a PageRank en el conectoma humano completo y en un subgrafo representativo de $10^4$ nodos para medir con precisión el consumo de memoria. Los resultados, resumidos en la Tabla \ref{table:efficiency_benchmark}, demuestran una ventaja drástica para el ORT.

\begin{table}[h!]
\centering
\caption{Benchmark de Eficiencia Computacional: ORT (Degree, P95) vs. PageRank (P95).}
\label{table:efficiency_benchmark}
\begin{tabular}{@{}llrrr@{}}
\toprule
\textbf{Alcance} & \textbf{Operador} & \textbf{Tiempo (s)} & \textbf{RAM Pico (MB)} & \textbf{Nodos en Núcleo} \\ \midrule
Conectoma & \textbf{ORT (Degree)} & \textbf{0.40} & \textbf{Mínimo}* & \textbf{8,593} \\
Completo & PageRank & 38.56 & $>$200 MB* & 8,588 \\
\midrule
Subgrafo & \textbf{ORT (Degree)} & \textbf{0.036} & \textbf{1.1} & \textbf{509} \\
($10^4$ nodos) & PageRank & 1.360 & 20.8 & 500 \\
\bottomrule
\multicolumn{5}{l}{\footnotesize{*Consumo de RAM en conectoma completo estimado; mínimo para ORT, alto para PageRank.}}
\end{tabular}
\end{table}

Como muestra la tabla, en el conectoma completo, el ORT es \textbf{$\sim$96 veces más rápido} que PageRank. En el subgrafo, donde la memoria pudo ser medida con precisión, el ORT no solo fue $\sim37$ veces más rápido, sino que consumió \textbf{$\sim$19 veces menos memoria RAM}. Si bien la ventaja en rendimiento de PageRank es marginal en términos absolutos ($\sim$1\% en F1-score), nuestros tests revelan que es estadísticamente significativa (p $<$ 0.01). Por tanto, la drástica ganancia en eficiencia y ahorro de recursos valida al ORT (Grado) no solo como un modelo biológicamente plausible, sino también como un poderoso **principio de parsimonia computacional**.


\section{Discusión}
Este trabajo partió de un análisis crítico de un modelo computacional de consolidación de memoria, el Operador de Refuerzo Topológico (ORT), con el objetivo de subsanar sus debilidades metodológicas y entender cómo la memoria funcional emerge de la estructura de una red, específicamente de su organización en un núcleo y una periferia (\textit{core/periphery}) \cite{borgatti2000}. El resultado de este proceso no ha sido una simple corrección, sino el descubrimiento de principios más profundos y matizados. A continuación, discutimos los dos hallazgos principales que se derivan de nuestros experimentos revisados.

\subsection{Del Artefacto de la Memoria Perfecta a la Dinámica de ''Masa Crítica ''}
El análisis preliminar del ORT mostraba una capacidad de memoria asociativa perfecta (Recall del 100\%) en redes como Pubmed. Sin embargo, la revisión crítica identificó correctamente que este resultado era un artefacto de la metodología de difusión y evaluación. Al implementar un protocolo de validación funcional robusto, basado en difusión normalizada y métricas F1-score, obtuvimos una visión mucho más realista. La ''memoria perfecta'' dio paso a una métrica de rendimiento robusta y sensible, que permitió realizar comparaciones significativas entre las distintas condiciones experimentales.

Esta nueva metodología nos llevó al primer hallazgo clave del estudio: en las redes de información (Cora, Citeseer y Pubmed), la eficacia de la memoria es \textbf{inversamente proporcional a la exclusividad del núcleo del engrama}. Contrario a la hipótesis inicial de que un núcleo de  ''élite'' (P95, el 5\% superior de hubs) sería el más robusto, los resultados demostraron consistentemente que un núcleo más grande de  ''masa crítica'' (P90, el 10\% superior) obtenía un rendimiento funcional significativamente mayor (ver Tabla \ref{table:percentile_comparison_f1}). Este  ''principio de masa crítica'' sugiere que, en estas redes, la resiliencia de la memoria no depende de la importancia individual extrema de unos pocos nodos, sino de la redundancia y la fuerza colectiva de un grupo más amplio de hubs.

\subsection{Especificidad de Dominio: Una Estrategia Dual para la Memoria}
El segundo hallazgo, y quizás el más sorprendente, surgió al aplicar nuestro protocolo validado a los conectomas biológicos. Esperábamos que el principio de  ''masa crítica '' se mantuviera. Sin embargo, los resultados mostraron exactamente lo contrario.

En el conectoma del macaco, y de forma aún más contundente en el conectoma humano, el núcleo de  ''élite'' (P95) demostró ser la estrategia funcionalmente óptima, alcanzando F1-scores de hasta 1.00 en el macaco y 87.4\% en el humano (ver Tabla \ref{table:human_connectome_results}). La estrategia P90, dominante en las redes de información, fue consistentemente inferior en las redes biológicas.

Este hallazgo revela una \textbf{especificidad de dominio} en la estrategia de consolidación de memoria. Sugiere una hipótesis novedosa: las redes biológicas, producto de millones de años de optimización evolutiva, pueden haber desarrollado una arquitectura de red mucho más eficiente. No necesitan la redundancia de una  ''masa crítica'' para asegurar la resiliencia, sino que pueden sostener una memoria robusta en un circuito de  ''super-hubs'' más pequeño, rápido y energéticamente menos costoso. En contraste, las redes de información, con una estructura más  ''ruidosa'' o menos optimizada, dependen de la fuerza bruta de un engrama más grande. Este principio de optimización diferencial entre dominios es una de las principales contribuciones de nuestro estudio, y es consistente con la literatura en neurociencia de redes que subraya la importancia funcional de los hubs cerebrales selectos y la organización multiescala del conectoma \cite{bassett2017, betzel2016}.

Este hallazgo de un núcleo de ``élite'' P95 como estrategia óptima en redes biológicas es, además, notablemente consistente con la literatura en neurociencia de redes. Investigaciones sobre el conectoma humano han identificado una estructura de ``club de ricos'' (\textit{rich-club}), donde los hubs cerebrales forman un núcleo densamente interconectado que actúa como una ``columna vertebral'' para la comunicación global \cite{van2011rich}. Es significativo que el tamaño de este núcleo, crucial para la integración de información, se estime en un porcentaje similar al 5\% de nodos que nuestro análisis funcional ha revelado como óptimo, reforzando la plausibilidad biológica del ORT.

\subsection{Especificidad Topológica y el Principio de Parsimonia Computacional}
Más allá del tamaño óptimo del engrama, nuestros resultados demuestran de forma concluyente que su eficacia funcional depende de la \textbf{organización topológica específica} de la red. El experimento de control con \textit{rewiring} en el conectoma humano fue particularmente revelador: la red ''desorganizada'', a pesar de conservar la misma distribución de grados, mostró una caída drástica en el rendimiento de la memoria (p $<$ 0.01). Notablemente, el propio algoritmo de \textit{rewiring} encontró dificultades para ejecutarse, un indicio de que la estructura del conectoma es altamente no-aleatoria y optimizada. Esto confirma que la memoria asociativa es una propiedad emergente de la configuración precisa de los hubs en la arquitectura cerebral.

Este hallazgo se complementa con la validación de un \textbf{principio de parsimonia computacional}. En todos los experimentos, la centralidad de Grado, un operador computacionalmente trivial ($O(m)$), demostró una eficacia funcional solo marginalmente inferior a la de PageRank, un algoritmo iterativo mucho más costoso ($O(k \cdot m)$) que se basa en multiplicaciones sucesivas de la matriz de la red \cite{gleich2015}. Si bien esta pequeña diferencia de rendimiento ($\sim$1\%) resultó ser estadísticamente significativa (p $<$ 0.01), nuestro benchmark en el conectoma humano cuantificó el coste de esa mejora marginal: el ORT basado en Grado fue \textbf{$\sim$96 veces más rápido}. Esto sugiere que los sistemas biológicos, en un claro ejemplo de la economía en la organización de redes cerebrales \cite{bullmore2012}, podrían haber favorecido mecanismos de consolidación ''baratos'' y locales que ofrecen casi toda la ventaja funcional sin el alto coste de cómputos globales.


\subsection{Implicaciones, Limitaciones y Direcciones Futuras}
\paragraph{Implicaciones en Neurociencia Computacional.}
En el ámbito de la neurociencia computacional, el ORT ofrece un marco cuantitativo innovador para estudiar la consolidación de memoria desde la estructura de red. Al modelar la memoria como una propiedad emergente de la conectividad topológica, este enfoque permite formalizar el proceso de consolidación sin recurrir a mecanismos sinápticos explícitos, proponiendo así una \textbf{hipótesis estructural de la memoria}: la persistencia de un engrama depende de la estabilidad y redundancia del núcleo topológico que lo sustenta.

Esta perspectiva puede facilitar el desarrollo de \textbf{biomarcadores topológicos} aplicables al análisis de conectomas humanos y animales. Medidas derivadas del ORT podrían emplearse para cuantificar la eficiencia de la consolidación en estados normales y patológicos, por ejemplo en trastornos donde se altera la conectividad funcional (como Alzheimer o epilepsia). De este modo, el modelo no sólo describe la memoria, sino que sugiere métricas observables que conectan estructura, función y patología.

Asimismo, el enfoque del ORT refuerza la visión de la \textbf{memoria como una propiedad multiescala}, emergente de interacciones locales pero gobernada por la organización global del grafo. Esta idea complementa los modelos clásicos de plasticidad hebbiana y los extiende a una escala mesoestructural, integrando la dinámica sináptica con la arquitectura global del cerebro.

\paragraph{Aplicaciones en Inteligencia Artificial.}
Para la IA, el paradigma \textit{aprender-consolidar} y el principio de parsimonia computacional refuerzan la viabilidad de métodos de aprendizaje más eficientes. El ORT puede considerarse una forma de \textit{graph pruning} o compresión de modelos post-entrenamiento, donde en lugar de reentrenar un modelo completo se identifica y refuerza una subred funcionalmente crítica.

Además, este enfoque ofrece implicaciones directas para el \textbf{aprendizaje continuo} (\textit{continual learning}), un ámbito en el que la preservación de representaciones previas sin reentrenamiento completo es esencial para evitar el \textit{catastrophic forgetting}. Integrar una fase de consolidación topológica como el ORT podría permitir que los modelos de aprendizaje profundo refuercen sus conexiones estructuralmente más estables, manteniendo la plasticidad para nuevas tareas sin comprometer la memoria previa.

De este modo, el ORT actúa no sólo como un operador teórico de consolidación, sino también como un mecanismo práctico para construir arquitecturas adaptativas y energéticamente eficientes, capaces de mantener un equilibrio óptimo entre plasticidad y estabilidad. Esta perspectiva abre la puerta a modelos de inteligencia artificial que emulen de manera más fiel la consolidación sináptica observada en sistemas biológicos.

La principal limitación de este estudio es que se basa en conectomas estáticos. Futuras investigaciones deberán incorporar \textbf{datos dinámicos y longitudinales}, como registros multimodales (fMRI, EEG) o datasets del Human Connectome Project (HCP), para evaluar si los engramas inducidos computacionalmente se correlacionan con patrones de activación temporal reales. Asimismo, será crucial probar el ORT en \textbf{entornos de plasticidad sináptica simulada}, donde la topología cambia con el aprendizaje, para verificar su validez en escenarios más cercanos a la fisiología cerebral.

Más allá de la validación en conectomas, este trabajo se conecta con estudios recientes sobre consolidación de memoria en modelos de grafos y aprendizaje continuo en IA \cite{benna2022, kong2023}. Estas investigaciones sugieren que la eficiencia en la consolidación no solo depende de la topología, sino también de mecanismos dinámicos de plasticidad y de prevención del \textit{catastrophic forgetting}, áreas donde el ORT podría integrarse de manera natural.


\section{Conclusión}
Este estudio ha presentado una validación rigurosa del Operador de Refuerzo Topológico (ORT), transformándolo de un modelo conceptual a un principio robusto, escalable y biológicamente plausible. A través de un proceso de revisión crítica, hemos desarrollado una metodología de validación funcional que nos ha permitido descubrir dos principios fundamentales sobre la memoria en redes complejas.

Primero, hemos demostrado una \textbf{especificidad de dominio} en la estrategia de consolidación: mientras que las redes de información maximizan la resiliencia de la memoria con núcleos de ''masa crítica'' (P90), las redes biológicas, más optimizadas, lo hacen con núcleos de ''élite'' más pequeños y eficientes (P95). Segundo, hemos validado un \textbf{principio de parsimonia computacional}, demostrando que operadores simples como la centralidad de Grado alcanzan una eficacia funcional comparable a la de algoritmos complejos como PageRank, pero con una ganancia drástica en eficiencia.

En conjunto, estos hallazgos establecen al ORT no solo como un algoritmo, sino como una lente para entender cómo los sistemas complejos optimizan la relación entre estructura, función y eficiencia. El trabajo contribuye a tender un puente cuantitativo entre la neurociencia y la inteligencia artificial, demostrando que principios simples basados en la topología de la red pueden explicar la emergencia de funciones cognitivas complejas como la memoria asociativa.

% --- AGRADECIMIENTOS (VERSIÓN MEJORADA Y FINAL) ---
\section*{Agradecimientos}
Este trabajo fue posible gracias a las herramientas de código abierto desarrolladas y mantenidas por la comunidad científica global. Un agradecimiento especial a los creadores de PyTorch, PyTorch Geometric, NetworkX y del ecosistema RAPIDS (cuDF, cuGraph) de NVIDIA, que permitieron el análisis eficiente de grafos a gran escala.

Mi inmensa gratitud a todos aquellos que trabajan para democratizar el acceso al conocimiento y a la investigación. Esta investigación también se benefició de la asistencia de modelos de lenguaje.

Este trabajo se realizó de manera completamente independiente, sin financiación institucional ni corporativa, demostrando que la investigación de frontera puede surgir también desde entornos abiertos y accesibles.

Finalmente, agradezco a The Network Data Repository (networkrepository.com) por proporcionar los datos de los conectomas biológicos utilizados en este estudio.

\vspace{1cm}

\begin{quote}
\textit{El presente estudio busca tender un puente entre la neurociencia y la inteligencia artificial. La comprensión de cómo estructuras simples generan funciones complejas es uno de los retos centrales de la ciencia moderna.}
\end{quote}


\section*{Licencia y Consideraciones Éticas}
Este trabajo se distribuye bajo la licencia: \\
\textbf{Creative Commons Attribution 4.0 International (CC BY 4.0)}. Esto significa que cualquier persona es libre de compartir (copiar y redistribuir el material en cualquier medio o formato) y adaptar (remezclar, transformar y construir sobre el material) para cualquier propósito, incluso comercial, bajo las siguientes condiciones: debe dar el crédito apropiado y proporcionar un enlace a la licencia.

Adicionalmente, como autor, declaro mi intención de que esta investigación contribuya al avance del conocimiento abierto y al bienestar social. En consecuencia, solicito que este trabajo no sea utilizado en aplicaciones destinadas a fines militares, sistemas de vigilancia masiva, o cualquier tecnología diseñada para el control social o la violación de los derechos humanos.


% --- DISPONIBILILAD DE DATOS Y CÓDIGO ---
\section*{Disponibilidad de Datos y Código}
El código fuente completo, implementado como notebooks de Google Colab autocontenidos y documentados, junto con todos los datos generados durante este estudio, están disponibles públicamente en el siguiente repositorio de GitHub para garantizar la total transparencia y reproducibilidad:\\
\url{https://github.com/NachoPeinador/Topological-Reinforcement-Operator}
\newpage


% --- BIBLIOGRAFÍA ---
\section*{Referencias}
\begin{thebibliography}{99}

\bibitem{sen2008collective}
P. Sen, G. M. Namata, M. Bilgic, L. Getoor, B. Gallagher, \& T. Eliassi-Rad, ``Collective classification in network data,'' \textit{AI Magazine}, vol. 29, no. 3, pp. 93-106, 2008.

\bibitem{josselyn2020}
S. A. Josselyn \& S. Tonegawa, ``Memory engrams: Storing information as physical constructs,'' \textit{Science}, vol. 367, no. 6473, eaaw4325, 2020.

\bibitem{hebb1949}
D. O. Hebb, \textit{The Organization of Behavior: A Neuropsychological Theory}. Wiley, 1949.

\bibitem{bronstein2017}
M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, \& P. Vandergheynst, ``Geometric deep learning: Going beyond euclidean data,'' \textit{IEEE Signal Processing Magazine}, vol. 34, no. 4, pp. 18-42, 2017.


\bibitem{nr-aaai15}
R. A. Rossi \& N. K. Ahmed, ``The network data repository with interactive graph analytics and visualization,'' en \textit{Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence}, 2015.

\bibitem{sporns2010}
O. Sporns, \textit{Networks of the Brain}. MIT Press, 2010.

\bibitem{hagmann2008}
P. Hagmann, M. Cammoun, X. Gigandet, et al., ``The human connectome: a complex network,'' \textit{PLoS ONE}, vol. 3, no. 7, e2640, 2008.

\bibitem{wu2020}
Z. Wu, S. Pan, F. Chen, et al., ``A comprehensive survey on graph neural networks,'' \textit{IEEE Transactions on Neural Networks and Learning Systems}, vol. 32, no. 1, pp. 4-24, 2021.

\bibitem{barabasi1999}
A.-L. Barabási \& R. Albert, ``Scale-Free Networks,'' \textit{Science}, vol. 286, no. 5439, pp. 509-512, 1999.

\bibitem{brandes2001}
U. Brandes, ``A faster algorithm for betweenness centrality,'' \textit{Journal of Mathematical Sociology}, vol. 25, no. 2, pp. 163-177, 2001.

\bibitem{bullmore2012}
E. Bullmore \& O. Sporns, ``The economy of brain network organization,'' \textit{Nature Reviews Neuroscience}, vol. 13, no. 5, pp. 336-349, 2012.

\bibitem{he2023}
Y. He, L. Zhu, Y. Zhu, \& F. Wu, ``Graph pruning for model compression,'' in \textit{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, 2023, pp. 21959-21969.

\bibitem{borgatti2000}
S. P. Borgatti \& M. G. Everett, ``Models of core/periphery structures,'' \textit{Social Networks}, vol. 21, no. 4, pp. 375--395, 2000.

\bibitem{bassett2017}
D. S. Bassett \& O. Sporns, ``Network neuroscience,'' \textit{Nature Neuroscience}, vol. 20, no. 3, pp. 353--364, 2017.

\bibitem{betzel2016}
R. F. Betzel \& D. S. Bassett, ``Multi-scale brain networks,'' \textit{NeuroImage}, vol. 160, pp. 73--83, 2016.

\bibitem{gleich2015}
D. F. Gleich, ``PageRank beyond the Web,'' \textit{SIAM Review}, vol. 57, no. 3, pp. 321--363, 2015.

\bibitem{mahadevan2022}
A. Mahadevan, J. Kim, \& O. Sporns, ``Rewiring connectomes: Implications for brain network organization,'' \textit{Nature Communications}, vol. 13, no. 1, 5324, 2022.

\bibitem{benna2022}
M. K. Benna \& S. Fusi, ``Computational principles of synaptic memory consolidation,'' \textit{Nature Neuroscience}, vol. 25, pp. 771–784, 2022.

\bibitem{kong2023}
L. Kong, X. Li, \& J. Y. Zhu, ``Graph-based continual learning: A survey,'' \textit{IEEE Transactions on Neural Networks and Learning Systems}, 2023.

\bibitem{van2011rich}
M. P. van den Heuvel \& O. Sporns, ``Rich-club organization of the human connectome,'' \textit{Journal of Neuroscience}, vol. 31, no. 44, pp. 15775-15786, 2011.


\end{thebibliography}

  



\end{document}