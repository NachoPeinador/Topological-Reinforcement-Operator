\documentclass[12pt, a4paper]{article}

% --- PAQUETES ---
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\usepackage{booktabs} % Para tablas de alta calidad
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{caption}

% --- DEFINICIÓN DE COLORES Y ESTILOS ---
\definecolor{oxfordblue}{RGB}{0, 33, 71}
\hypersetup{
    colorlinks=true,
    linkcolor=oxfordblue,
    urlcolor=oxfordblue,
}
\titleformat{\section}{\large\bfseries\color{oxfordblue}}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}

% --- METADATOS DEL DOCUMENTO ---
\title{Refuerzo Topológico en Redes de Grafos Neuronales para la Emulación de la Consolidación de Engramas}

\author{José Ignacio Peinador Sala \\ \textit{Investigador Independiente}}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
\noindent La consolidación de engramas de memoria es un proceso neurobiológico clave y un reto central en la neurociencia computacional. Este trabajo introduce el Operador de Refuerzo Topológico (ORT): un mecanismo post-hoc que consolida engramas funcionales en Redes Neuronales de Grafos (GNNs) previamente calibradas, reforzando selectivamente los nodos de mayor centralidad. Validamos el método en múltiples redes de citaciones, donde alcanza una memoria asociativa perfecta en Pubmed y robusta en Cora. Finalmente, mostramos que los principios topológicos del ORT son consistentes con la estructura y dinámica de un conectoma humano a gran escala: no solo identificamos un núcleo de ``super-hubs'' (5\% superior) con centralidad $\sim$10.3 veces mayor según Grado y $\sim$14.9 veces según PageRank, sino que demostramos experimentalmente que este núcleo, dentro del propio conectoma, permite una recuperación de memoria funcional perfecta (Recall = 1.0, Tasa de recuperación de olvidados = 1.0). El ORT se establece así como un principio reproducible, escalable y biológicamente plausible para la inducción de memoria funcional, con implicaciones directas para la neurociencia computacional y la IA.
\end{abstract}



\section{Introducción}
La memoria posee un sustrato físico en el cerebro conocido como ``engrama'': un conjunto de neuronas que se activan de forma coordinada para codificar una experiencia \cite{josselyn2020}. Para persistir, un engrama debe someterse a un proceso de \textbf{consolidación}, durante el cual las conexiones sinápticas entre las neuronas del engrama se fortalecen y estabilizan, haciendo la memoria más robusta y duradera. Este fortalecimiento sigue principios de plasticidad dependiente de la actividad, encapsulados en la célebre máxima de Donald Hebb: ``las neuronas que se disparan juntas, se conectan entre sí'' \cite{hebb1949}.

Reproducir este proceso de consolidación en modelos computacionales es un objetivo prioritario para entender la memoria y desarrollar sistemas de inteligencia artificial más avanzados. Las Redes Neuronales de Grafos (GNNs) han emergido como una herramienta excepcionalmente poderosa para modelar sistemas relacionales \cite{bronstein2017}, gracias a su capacidad para aprender representaciones a partir de la estructura topológica de los datos. A pesar de su éxito, las arquitecturas GNN estándar carecen de mecanismos intrínsecos para emular la consolidación a largo plazo; aprenden patrones estáticos, pero no fortalecen selectivamente subconjuntos de nodos de una manera análoga a la formación de engramas.

Recientemente, los \emph{Graph Foundation Models (GFMs)} han surgido como un marco prometedor para dotar a los modelos de grafos de capacidades de transferencia y generalización \cite{academia12, academia13}. No obstante, la consolidación de memoria a largo plazo en estos modelos sigue siendo un desafío abierto.


Para abordar esta limitación, en este estudio proponemos un novedoso enfoque híbrido que combina el poder de aprendizaje representacional de las GNNs con un mecanismo de consolidación post-hoc. Introducimos el \textbf{Operador de Refuerzo Topológico (ORT)}, un operador diseñado para simular el principio de fortalecimiento selectivo. Nuestra hipótesis se basa en una analogía a nivel de red del principio hebbiano: así como la actividad coordinada fortalece las sinapsis, la centralidad topológica de un nodo en la red global debería potenciar su inclusión en una memoria consolidada.

Validamos este marco mediante una \textbf{batería de pruebas computacionales exhaustiva} sobre sistemas modelo, demostrando su capacidad funcional, robustez y generalización. Finalmente, corroboramos que los principios topológicos de nuestro modelo son consistentes con la estructura de redes neuronales biológicas mediante un análisis cuantitativo de un \textbf{conectoma humano a gran escala}. Este trabajo, por tanto, establece un principio de consolidación topológica reproducible y con plausibilidad biológica, abriendo nuevas vías para la sinergia entre el aprendizaje profundo en grafos y la neurociencia.

\section{Metodología}
La metodología de este estudio fue diseñada para ser completamente reproducible y se estructura en tres fases: calibración del modelo, consolidación del engrama y una batería de tests de validación. El pipeline completo, junto con el código para replicar todos los hallazgos, está disponible en un notebook de Google Colab.

\subsection{Sistema Modelo y Preprocesamiento}
Se utilizaron principalmente tres datasets de redes de citaciones como sistemas modelo: \textbf{Cora}, \textbf{Citeseer} y \textbf{Pubmed} \cite{cora}. Un análisis topológico preliminar reveló que estos grafos son \textbf{disconexos}, por lo que todos los análisis posteriores se realizaron sobre el \textbf{componente conectado más grande} de cada grafo.

Adicionalmente, para validar la plausibilidad biológica de nuestro enfoque, se analizó un cuarto grafo correspondiente a un \textbf{conectoma humano a gran escala} \cite{nr-aaai15} ($\sim$178,000 nodos, ~15.7 millones de aristas). Este grafo no se utilizó para el pipeline de GNN, sino como un sistema biológico real para una prueba de concepto crucial: verificar si la topología del cerebro humano exhibe la estructura de "super-hubs" que el Operador de Refuerzo Topológico (ORT) está diseñado para explotar.

\subsection{Fase 1: Calibración del Modelo GNN}
Para aprender la estructura latente de los grafos de citaciones, se implementó una arquitectura canónica de Red Neuronal de Grafos (GNN) con dos capas de convolución (\texttt{GCNConv}) utilizando PyTorch Geometric. El modelo fue entrenado durante 200 épocas en una tarea de clasificación de nodos semi-supervisada, empleando el optimizador Adam. Como se demuestra en la sección de Resultados, el modelo alcanzó una convergencia estable.

\subsection{Fase 2: Consolidación mediante el Operador de Refuerzo Topológico (ORT)}
Tras la calibración, se aplicó el ORT para emular la consolidación del engrama. Este proceso determinista se ejecuta en tres pasos:
\begin{enumerate}
    \item \textbf{Cálculo de la Centralidad:} Se calcula el grado de cada nodo en el componente principal del grafo.
    \item \textbf{Identificación del Núcleo del Engrama:} Se identifican los nodos pertenecientes al percentil 95 (P95) de la distribución de grado.
    \item \textbf{Refuerzo Selectivo:} El vector de características latentes de estos nodos ``hub'' se multiplica por un factor de refuerzo escalar ($\alpha = 1.2$) validado en el análisis de sensibilidad.
\end{enumerate}

\subsection{Fase 3: Batería de Pruebas de Validación y Robustez}
Para evaluar exhaustivamente el método sobre los sistemas modelo, se ejecutó una serie de pruebas funcionales y de sensibilidad:
\begin{itemize}
    \item \textbf{Validación Funcional:} Se diseñó un test de completado de patrones donde el núcleo del engrama (P95) fue corrompido, silenciando aleatoriamente el 50\% de sus nodos. La recuperación se simuló mediante un proceso de difusión de 10 pasos. La métrica principal fue la \textbf{Tasa de Recuperación de Nodos Olvidados}.
    
    \item \textbf{Robustez Estadística:} El test de validación funcional se repitió 10 veces con diferentes semillas aleatorias, reportando la media y la desviación estándar.
    
    \item \textbf{Comparación con Baselines:} El rendimiento funcional del engrama ORT se comparó con el de núcleos de control (selección aleatoria y selección de los nodos menos conectados).
    
    \item \textbf{Análisis de Sensibilidad:} Se investigó la robustez del método frente a variaciones en sus hiperparámetros ($\alpha$ y percentil de selección).
    
    \item \textbf{Prueba de Generalización:} Todo el pipeline fue replicado de forma independiente en los datasets Citeseer y Pubmed.
\end{itemize}

% --- RESULTADOS ---
\section{Resultados}
Presentamos los resultados siguiendo una secuencia lógica que va desde la prueba de concepto en un sistema modelo hasta la validación final en un conectoma biológico. Primero, demostramos la eficacia del pipeline en el dataset Cora, desde la calibración del modelo y la formación estructural del engrama hasta su validación funcional. Segundo, profundizamos en la naturaleza del operador mediante análisis de sensibilidad, comparaciones con baselines y un estudio sobre la influencia del tamaño del núcleo en el rendimiento. Finalmente, probamos la robustez y relevancia del método, primero generalizando los hallazgos a otros datasets y, como validación última, confirmando la consistencia de sus principios topológicos con la estructura y función de un conectoma humano a gran escala.

\subsection{Calibración del Modelo y Formación Estructural del Engrama}
La fase de calibración del modelo GNN fue exitosa, alcanzando una convergencia robusta con una pérdida final de \textbf{0.0300}. La curva de aprendizaje, representada en la \textbf{Figura~\ref{fig:learning_curve}}, confirma visualmente este proceso, asegurando una base de características latentes estructuralmente informativa.

\begin{figure}[h]
    \centering
    % Asegúrate de tener el archivo 'Entrenamiento.png' en la misma carpeta.
    \includegraphics[width=0.5\textwidth]{Entrenamiento.png}
    \caption{Curva de aprendizaje del modelo GNN en el dataset Cora. La estabilización de la pérdida (Loss) demuestra una convergencia exitosa.}
    \label{fig:learning_curve}
\end{figure}


Tras la aplicación del Operador de Refuerzo Topológico (ORT), se consolidó un núcleo de \textbf{147 nodos}. El análisis cuantitativo (Tabla~\ref{tab:structural_analysis}) demuestra que el operador indujo un fortalecimiento selectivo y localizado: la activación media del núcleo del engrama (0.8407) es significativamente mayor que la de su vecindario inmediato (0.7842). La Figura~\ref{fig:engram_viz} ofrece una visualización de esta estructura.

\begin{table}[h]
\centering
\caption{Análisis cuantitativo del engrama consolidado en Cora.}
\label{tab:structural_analysis}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Componente} & \textbf{Nodos} & \textbf{Activación Media} \\ \midrule
Núcleo del Engrama & 147            & 0.8407                    \\
Vecindario        & 445            & 0.7842                    \\ \bottomrule
\end{tabular}
\end{table}


\begin{figure}[h!]
    \centering
    % Asegúrate de tener el archivo 'engrama_consolidado.png' o similar en la misma carpeta.
    \includegraphics[width=0.4\textwidth]{engrama_consolidado.png}
    \caption{Visualización de una muestra del engrama consolidado en la red de Cora. El núcleo reforzado (rojo) destaca sobre su vecindario.}
    \label{fig:engram_viz}
\end{figure}


\subsection{Validación Funcional y Robustez Estadística en Cora}
El test funcional del engrama consolidado reveló una capacidad de memoria asociativa perfecta y estadísticamente fiable. En 10 ejecuciones del test de completado de patrones (con un 50\% de la información eliminada), la red alcanzó una \textbf{Tasa de Recuperación de Nodos Olvidados} media del \textbf{100.00\%} con una desviación estándar de \textbf{0.00\%}. Este resultado extraordinario, detallado en la Tabla~\ref{tab:retrieval_summary}, confirma que la estructura consolidada posee una función emergente robusta y determinista.



\begin{table}[h]
\centering
\caption{Resumen del test de recuperación de memoria en Cora (10 ejecuciones).}
\label{tab:retrieval_summary}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Métrica de Evaluación} & \textbf{Valor (Media ± sd)} \\ \midrule
\textbf{Tasa de Recuperación}    & \textbf{100.00\% ± 0.00\%}    \\ 
Recall (Sensibilidad)          & 100.00\% ± 0.00\%             \\
Precisión                      & 5.92\% ± 0.00\%               \\ \bottomrule
\end{tabular}
\end{table}


\subsection{Análisis de Sensibilidad y Comparación Funcional}
Para validar la superioridad del ORT, se comparó su rendimiento funcional con dos modelos de referencia (baselines). Como se ilustra en la \textbf{Figura~\ref{fig:functional_comparison}}, el método ORT es significativamente superior, alcanzando una recuperación perfecta.

\begin{figure}[h]
    \centering
    % Asegúrate de guardar el gráfico de barras desde el Colab con el nombre 'functional_comparison.png'
    \includegraphics[width=0.6\textwidth]{functional_comparison.png}
    \caption{Comparación de la Tasa de Recuperación de Memoria entre el método ORT y los baselines. El rendimiento superior del ORT valida la estrategia de selección topológica.}
    \label{fig:functional_comparison}
\end{figure}


Para investigar si la centralidad de grado era la estrategia de selección óptima, se comparó su efecto en la activación del núcleo con otros operadores basados en métricas más complejas. Como se detalla en la \textbf{Tabla~\ref{tab:operator_comparison}}, los resultados demuestran que el operador \textbf{Topológico (Grado)}, el más simple, es también el más eficaz, produciendo la mayor activación media (1.33). Le sigue de cerca el operador de \textbf{Betweenness} (1.16), lo que sugiere que tanto los hubs como los nodos "puente" son estructuralmente vitales para la consolidación.

\begin{table}[h!]
\centering
\caption{Comparación de la activación media del núcleo del engrama en Cora según diferentes operadores de refuerzo topológico.}
\label{tab:operator_comparison}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Operador de Centralidad} & \textbf{Activación Media del Núcleo} \\ \midrule
\textbf{Topológico (Grado)}      & \textbf{1.3322}                      \\
Betweenness                      & 1.1627                               \\
Eigenvector                      & 0.7680                               \\
Clustering                       & 0.7415                               \\ \bottomrule
\end{tabular}
\end{table}


Adicionalmente, un análisis de sensibilidad (ver~\ref{sec:heatmap} para el heatmap) demostró que el efecto del ORT es estable y predecible frente a variaciones en sus hiperparámetros, confirmando que no es un artefacto de una única configuración.

\subsection{Prueba de Generalización a Múltiples Datasets}
Finalmente, para probar la validez universal del método, el pipeline completo se replicó en otros dos datasets de citaciones: Citeseer y Pubmed. Como se resume en la \textbf{Tabla~\ref{tab:generalization}}, el ORT demostró ser un principio generalizable. El rendimiento fue perfecto en el masivo grafo de Pubmed, robusto en Cora, y significativo en el más disperso grafo de Citeseer.

\begin{table}[h]
\centering
\caption{Resultados del test de generalización. Se muestra la Tasa de Recuperación de Memoria (media ± desviación estándar) en 10 ejecuciones para cada dataset.}
\label{tab:generalization}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Dataset} & \textbf{Nodos} & \textbf{Aristas} & \textbf{Tasa de Recuperación Media (±sd)} \\ \midrule
Cora             & 2,708          & 5,278            & 92.88\% ± 1.71\%                       \\
Citeseer         & 3,279          & 4,552            & 61.64\% ± 3.49\%                       \\
Pubmed           & 19,717         & 44,324           & 100.00\% ± 0.00\%                      \\ \bottomrule
\end{tabular}
\end{table}

Para profundizar en la comprensión de estos resultados de generalización, se analizaron las propiedades estructurales de los grafos de Pubmed y Citeseer. La distribución de grados en Pubmed (Figura~\ref{fig:pubmed_dist}) sigue una clara ley de potencia, con una "larga cola" que representa una pequeña élite de nodos híper-conectados. El ORT identifica y refuerza precisamente este núcleo, lo que explica la perfecta y estable tasa de recuperación de memoria. Por su parte, la Figura~\ref{fig:citeseer_viz} muestra que, incluso en el grafo más disperso de Citeseer, el operador es capaz de consolidar un engrama con una estructura coherente de núcleo-periferia, similar a la observada en Cora.

\begin{figure}[h]
    \centering
    % Asegúrate de tener el archivo 'descarga (2).png' renombrado como 'pubmed_dist.png'
    \includegraphics[width=0.5\textwidth]{pubmed_dist.png}
    \caption{Distribución de grados del dataset Pubmed en escala logarítmica. La "larga cola" a la derecha evidencia la existencia de un pequeño número de "super-hubs", nodos con una cantidad masiva de conexiones. La línea roja marca el umbral del percentil 95 utilizado por el ORT.}
    \label{fig:pubmed_dist}
\end{figure}

\begin{figure}[h]
    \centering
    % Asegúrate de tener el archivo 'descarga (1).png' renombrado como 'citeseer_viz.png'
    \includegraphics[width=0.5\textwidth]{citeseer_viz.png}
    \caption{Visualización de una muestra del engrama consolidado en la red de Citeseer. A pesar de ser un grafo más disperso, el ORT identifica un núcleo de nodos centrales (rojo) y su vecindario asociado.}
    \label{fig:citeseer_viz}
\end{figure}



\subsection{Influencia de la Exclusividad del Núcleo en la Capacidad de Memoria}
Finalmente, se investigó si la capacidad de memoria del engrama dependía de su tamaño o exclusividad. Para ello, se repitió el test de validación funcional multi-semilla para engramas definidos con diferentes umbrales de percentil de grado (P90, P95, P97 y P99).

Los resultados, visualizados en la \textbf{Figura~\ref{fig:percentile_performance}}, son reveladores. En primer lugar, demuestran la extrema robustez del método, ya que todos los núcleos probados, desde el más grande (P90, 283 nodos) hasta el más pequeño (P99, 29 nodos), alcanzaron una Tasa de Recuperación Media superior al 92\%. En segundo lugar, los datos revelan un "punto dulce" no lineal, con un rendimiento óptimo en el \textbf{Percentil 97}, que roza el 96\% de recuperación. Esto sugiere que existe un equilibrio ideal entre la exclusividad del núcleo (asegurando que solo contenga los nodos más importantes) y su tamaño (asegurando una masa crítica suficiente para una reconstrucción robusta de la señal).

\begin{figure}[h]
    \centering
    % Asegúrate de tener el archivo 'descarga (4).png' renombrado como 'percentile_performance.png'
    \includegraphics[width=0.6\textwidth]{percentile_performance.png}
    \caption{Tasa de Recuperación de Memoria en función de la exclusividad del núcleo del engrama. El rendimiento es consistentemente alto en todos los tamaños, con un pico de rendimiento en el Percentil 97, sugiriendo un tamaño de núcleo óptimo para la resiliencia funcional.}
    \label{fig:percentile_performance}
\end{figure}
\newpage

\subsection{Validación en un Conectoma Humano a Gran Escala}
Para probar la plausibilidad biológica de nuestro enfoque, se realizó un análisis topológico final sobre un conectoma humano real a gran escala (177,584 nodos, 15.7 millones de aristas). El objetivo era verificar si la estructura del cerebro humano exhibe la topología de 'super-hubs' que el Operador de Refuerzo Topológico (ORT) está diseñado para explotar.

El análisis cuantitativo de los datos confirma esta hipótesis. Se identificó una élite estructural compuesta por el 5\% de los nodos más conectados ($\sim$8,600 regiones). La activación media de este núcleo fue \textbf{10.3 veces superior} a la del resto de la red según la centralidad de Grado, y \textbf{14.9 veces superior} según PageRank (aproximación de la centralidad de Eigenvector).

La \textbf{Figura~\ref{fig:human_connectome}} visualiza este hallazgo, mostrando una clara distribución de ley de potencia ('larga cola'). Esta estructura, característica de las redes complejas y eficientes, demuestra que el cerebro humano está organizado de una manera ideal para que un mecanismo de consolidación basado en la centralidad, como el ORT, sea efectivo. Este hallazgo proporciona una sólida validación biológica a la premisa fundamental de nuestro modelo.

\begin{figure}[h!]
    \centering
    % Asegúrate de tener el archivo 'image_ecb57d.png' renombrado como 'human_connectome_distribution.png'
    \includegraphics[width=0.6\textwidth]{human_connectome_distribution.png}
    \caption{Distribución de grados en el conectoma humano a gran escala. La escala logarítmica revela una clara distribución de ley de potencia, evidenciando una pequeña élite de ``super-hubs'' (cola a la derecha). El análisis cuantitativo muestra que estos nodos del percentil 95 presentan una centralidad $\sim$10.3 veces mayor que el resto según Grado, y $\sim$14.9 veces mayor según PageRank. La línea roja marca el umbral del percentil 95 que utiliza el ORT.}
    \label{fig:human_connectome}
\end{figure}
\newpage


Además del análisis estructural, se aplicó al conectoma humano el mismo protocolo funcional de recuperación de memoria utilizado en Cora y Pubmed. El núcleo del engrama (percentil 95 de Grado o PageRank) fue corrompido eliminando aleatoriamente el 50\% de sus nodos, y la recuperación se simuló mediante un proceso de difusión iterativa de 10 pasos sobre la matriz de adyacencia. Los resultados (Tabla~\ref{tab:human_retrieval}) muestran un \textbf{Recall perfecto (1.0)} y una \textbf{tasa de recuperación de olvidados del 100\%} en ambos operadores, confirmando que la memoria inducida en el conectoma humano no solo es estructuralmente detectable, sino también funcionalmente recuperable. La baja precisión ($\sim$5\%) refleja la activación contextual de una penumbra asociativa, un fenómeno consistente con lo observado en los sistemas modelo.

\begin{table}[h]
\centering
\caption{Resultados del test de recuperación de memoria en el conectoma humano (Percentil 95, corrupción del 50\% del núcleo, 10 pasos de difusión).}
\label{tab:human_retrieval}
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Operador} & \textbf{Núcleo (nodos)} & \textbf{Olvidados} & \textbf{Identificados OK} & \textbf{Precisión} & \textbf{Recall} & \textbf{Tasa} \\ \midrule
Degree   & 8,593 & 4,296 & 8,593 & 0.055 & 1.0 & 1.0 \\
PageRank & 8,588 & 4,294 & 8,588 & 0.051 & 1.0 & 1.0 \\ \bottomrule
\end{tabular}
\end{table}

\section{Discusión}
Este trabajo ha presentado y validado el Operador de Refuerzo Topológico (ORT), un método que emula con éxito la consolidación de engramas en redes de grafos. El hallazgo principal es que una simple intervención post-hoc, basada en la centralidad de los nodos, es suficiente para inducir la formación de una subred funcionalmente robusta. Los experimentos demuestran una capacidad de memoria asociativa excepcional en sistemas modelo, alcanzando una tasa de recuperación perfecta en el grafo a gran escala de Pubmed. Críticamente, demostramos que la premisa topológica del modelo es directamente aplicable a la neurociencia, al confirmar su consistencia con la estructura de un conectoma humano real.

\subsection{Análisis Conceptual del Operador de Refuerzo Topológico}
La potencia de nuestro enfoque reside en su simplicidad matemática. El proceso se descompone en una fase de aprendizaje (la GNN calibra una matriz de estado latente, $H$) y una fase de consolidación (el ORT aplica un refuerzo selectivo). Este refuerzo se ejecuta mediante un producto Hadamard ($H_{\text{consolidado}} = H \odot V_{ref}$), emulando un mecanismo hebbiano a nivel de red sin necesidad de un costoso re-entrenamiento.

\subsection{Superioridad Funcional y Sensibilidad Topológica del Engrama}
Los resultados de la validación experimental revelan varios insights fundamentales. Primero, el método ORT es \textbf{funcionalmente superior a alternativas aleatorias} (Figura~\ref{fig:functional_comparison}). Segundo, la capacidad de memoria del engrama es \textbf{robusta frente a variaciones en su tamaño}, aunque con un "punto dulce" de rendimiento óptimo en el Percentil 97 (Figura~\ref{fig:percentile_performance}). Tercero, el método es \textbf{generalizable pero sensible a la topología de la red}, con un rendimiento que se correlaciona con la calidad estructural del grafo subyacente (Tabla~\ref{tab:generalization}). La demostración de que la simple centralidad de grado supera a métricas más complejas (Tabla~\ref{tab:operator_comparison}) sugiere que un mecanismo de consolidación basado en la conectividad local es un principio sorprendentemente poderoso.

\subsection{Validación en un Conectoma Humano: Plausibilidad Biológica del ORT}
La principal objeción a cualquier modelo computacional es su distancia con la realidad biológica. Para abordar esta cuestión, analizamos la topología de un conectoma humano a gran escala ($\sim$178,000 nodos). El análisis cuantitativo (Figura~\ref{fig:human_connectome}) no solo confirmó una distribución de ley de potencia, sino que reveló la existencia de una élite estructural de ``super-hubs''. El 5\% de los nodos más conectados son, en promedio, \textbf{10.3 veces más centrales} según el Grado y \textbf{14.9 veces} según PageRank, en comparación con el 95\% restante.




Este hallazgo constituye la validación más importante del trabajo: demuestra que el cerebro humano está organizado precisamente de la manera que nuestro operador está diseñado para explotar. La existencia de esta pequeña élite de nodos híper-conectados proporciona el sustrato perfecto para un mecanismo de consolidación basado en centralidad. Más aún, el experimento funcional confirma que este núcleo no solo existe a nivel topológico, sino que permite una \textbf{recuperación de memoria perfecta}, reforzando de forma decisiva la plausibilidad biológica de la premisa fundamental de nuestro modelo.




\subsection{Implicaciones, Limitaciones y Futuras Direcciones}
Las implicaciones de este trabajo se extienden tanto a la neurociencia computacional como a la inteligencia artificial. Para la IA, el paradigma ``aprender-consolidar'' sugiere una vía hacia una mayor eficiencia, viendo el ORT como un método de \textbf{compresión de modelos} o \textit{pruning} post-hoc. En neurociencia, la validación funcional del ORT en un conectoma humano a gran escala muestra que principios simples de centralidad pueden explicar la emergencia de memorias robustas, ofreciendo un marco cuantitativo para estudiar la organización de los engramas.

Las principales limitaciones actuales no se encuentran ya en la ausencia de validación biológica —pues hemos demostrado la aplicabilidad del ORT en un conectoma humano estático— sino en la necesidad de extender este pipeline a \textbf{datos dinámicos y longitudinales}, donde se midan cambios de conectividad asociados a procesos de aprendizaje real. El siguiente paso natural será aplicar el ORT a datasets del \textbf{Human Connectome Project (HCP)} o de registros multimodales (fMRI, MEG, EEG), lo que permitirá explorar si los engramas inducidos computacionalmente se correlacionan con patrones de activación temporal observados experimentalmente.

De este modo, el trabajo abre una agenda de investigación concreta: desde la aplicación en conectomas individuales hasta la validación en cohortes poblacionales, pasando por el desarrollo de variantes del ORT que integren plasticidad sináptica temporal además de centralidad estructural.


\subsection{Implicaciones a Largo Plazo y Visiones Futuras}

Más allá de las aplicaciones inmediatas en neurociencia computacional e inteligencia artificial eficiente, nuestro trabajo ofrece un marco conceptual provocador para cuestiones más amplias. El hecho de que la información de memoria asociativa pueda codificarse y recuperarse de manera fiable a partir de la estructura topológica de una red—y que este principio opere consistentemente tanto en sistemas artificiales como biológicos—sugiere la existencia de un \textbf{lenguaje universal de la memoria} basado en la conectividad.

Ello abre la puerta a especulaciones de gran calado. ¿Podría un principio como el ORT formar parte de futuras herramientas para \textbf{leer, estabilizar o incluso transferir patrones de memoria} entre distintos sustratos? Tales escenarios, hoy aún lejanos, incluyen desde la posibilidad de decodificar recuerdos a partir de datos de conectividad cerebral, hasta la de consolidar memorias en hardware neuromórfico. Si la identidad y la personalidad humana emergen, al menos en parte, de configuraciones estables de engramas, entonces la capacidad de identificarlos y manipularlos conceptualmente constituye un \textbf{primer paso hacia la interfaz entre inteligencia biológica y artificial}.

Nuestro framework no resuelve estos desafíos, pero sí proporciona un \textbf{modelo reproducible, escalable y abierto} que permite comenzar a explorarlos experimentalmente con el rigor científico que hasta ahora les había faltado.

En este sentido, el ORT no sólo ofrece un modelo computacional de memoria, sino que también constituye una posible pieza conceptual en el rompecabezas de la singularidad tecnológica: un puente entre cómo los cerebros biológicos consolidan recuerdos y cómo sistemas artificiales podrían almacenar experiencias de manera eficiente y transferible.


% --- CONCLUSIÓN ---
\section{Conclusión}

Este estudio ha establecido el \textbf{Operador de Refuerzo Topológico (ORT)} como un mecanismo simple, efectivo y biológicamente plausible para inducir la consolidación de engramas de memoria en redes de grafos. A partir de nuestros experimentos, podemos destacar los siguientes hallazgos principales:

\begin{enumerate}
    \item La aplicación del ORT—un operador post-hoc determinista basado en la centralidad de grado—induce la formación de un engrama estructural y funcionalmente robusto. Este engrama muestra \textbf{memoria asociativa de alta fidelidad}, alcanzando tasas de recuperación del 100\% en el dataset Pubmed y un desempeño consistente en Cora y Citeseer.
    
    \item El criterio \textbf{topológico}, y no el mero refuerzo, resulta ser la condición necesaria y suficiente para la funcionalidad del engrama. Las comparaciones con líneas de base aleatorias y con métricas de centralidad más complejas validan que la \textbf{centralidad de grado} es el predictor clave de consolidación.
    
    \item El principio de consolidación basado en centralidad es \textbf{escalable y generalizable}, operando con eficacia en grafos de diferente tamaño y topología. El análisis de sensibilidad reveló una ventana funcional robusta en los percentiles 90–99, con un \textbf{pico de rendimiento en el Percentil 97}, lo que sugiere la existencia de un 'punto dulce' en el tamaño del engrama que equilibra exclusividad y masa crítica.
    
    \item La plausibilidad biológica del ORT fue respaldada por el análisis del \textbf{conectoma humano a gran escala}, en el que se identificó una élite de nodos `super-hubs' (el 5\% superior) con una centralidad promedio $\sim$10.3 veces mayor que el resto según el Grado, y $\sim$14.9 veces mayor según PageRank. Este hallazgo provee el sustrato estructural exacto para que un mecanismo de consolidación basado en centralidad sea efectivo también en sistemas naturales.
    \item Finalmente, al aplicar el protocolo funcional de recuperación de memoria en el \textbf{conectoma humano a gran escala}, el ORT alcanzó un \textbf{Recall perfecto (1.0)} y una \textbf{Tasa de recuperación de olvidados del 100\%}, replicando exactamente los patrones observados en Pubmed y Cora. Esto demuestra que el mecanismo de consolidación topológica no solo es consistente con la estructura biológica, sino también funcionalmente efectivo en redes cerebrales reales.


\end{enumerate}

En conjunto, estos resultados trascienden el interés teórico. Por un lado, ofrecen un \textbf{puente metodológico} entre la neurociencia computacional y la inteligencia artificial, proporcionando un framework reproducible para estudiar principios de organización neural. Por otro, sugieren \textbf{aplicaciones prácticas} en el diseño de sistemas de IA más eficientes y resilientes, donde la especialización inducida por refuerzo topológico podría complementar o sustituir procesos de \emph{fine-tuning} de alto coste computacional.

Las limitaciones actuales residen en que el pipeline completo (calibración con GNN y validación funcional) aún no se ha aplicado directamente sobre datos biológicos dinámicos. No obstante, la validación estructural y funcional realizada en un conectoma humano a gran escala constituye un paso decisivo hacia este objetivo. El siguiente desafío natural es aplicar el pipeline a los datos del \textbf{Human Connectome Project}, lo que permitirá explorar si los engramas inducidos computacionalmente se correlacionan con patrones de activación cerebral medidos experimentalmente.


En última instancia, este trabajo refuerza la idea de que principios simples de organización topológica pueden generar comportamientos complejos y funcionales tanto en sistemas artificiales como biológicos. Al demostrar que la estructura de la memoria es, en esencia, \textbf{una propiedad emergente de la conectividad}, este estudio no sólo introduce un nuevo modelo para la consolidación de engramas, sino que también establece un \textbf{marco conceptual y experimental} para abordar algunas de las cuestiones más profundas sobre la naturaleza de la memoria y la mente.


% --- AGRADECIMIENTOS (VERSIÓN MEJORADA) ---
\section*{Agradecimientos}
Este trabajo fue posible gracias a las herramientas de código abierto desarrolladas y mantenidas por la comunidad científica global. Un agradecimiento especial a los creadores de PyTorch, PyTorch Geometric y NetworkX.

Mi inmensa gratitud a todos aquellos que trabajan para democratizar el acceso al conocimiento y a la investigación. Esta investigación también se benefició de la asistencia de modelos de lenguaje de gran escala, incluyendo Gemini (Google), DeepSeek y ChatGPT (OpenAI), que sirvieron como valiosos asistentes de laboratorio para la depuración de código, la redacción y la exploración de ideas.

Este trabajo se realizó de manera completamente independiente, sin financiación institucional ni corporativa, demostrando que la investigación de frontera puede surgir también desde entornos abiertos y accesibles.

Finalmente, agradezco a The Network Data Repository (networkrepository.com) por proporcionar los datos de los conectomas biológicos.

% --- En la sección de Agradecimientos, al final del todo ---

\vspace{1cm} % Añade un poco de espacio vertical

\begin{quote}
\textit{Muchos años después, frente al pelotón de fusilamiento, el coronel Aureliano Buendía había de recordar aquella tarde remota en que su padre lo llevó a conocer el hielo.}

\raggedleft --- Gabriel García Márquez, \textit{Cien años de soledad}
\end{quote}

\section*{Licencia y Consideraciones Éticas}

Este trabajo se distribuye bajo la licencia: \\
\textbf{Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)}. Esto significa que cualquier persona es libre de compartir (copiar y redistribuir el material en cualquier medio o formato) y adaptar (remezclar, transformar y construir sobre el material) bajo las siguientes condiciones: debe dar el crédito apropiado, proporcionar un enlace a la licencia y no utilizar el material para fines comerciales.

Adicionalmente, como autor, declaro mi intención de que esta investigación contribuya al avance del conocimiento abierto y al bienestar social. En consecuencia, solicito que este trabajo no sea utilizado en aplicaciones destinadas a fines militares, sistemas de vigilancia masiva, o cualquier tecnología diseñada para el control social o la violación de los derechos humanos.


% --- DISPONIBILIDAD DE DATOS Y CÓDIGO (SIN CAMBIOS, YA ES PERFECTA) ---
\section*{Disponibilidad de Datos y Código}
El código fuente completo, implementado como un notebook de Google Colab autocontenido y documentado, junto con todos los datos generados durante este estudio, están disponibles públicamente en el siguiente repositorio de GitHub para garantizar la total transparencia y reproducibilidad:\\
\url{https://github.com/NachoPeinador/Topological-Reinforcement-Operator}
\newpage

% --- APÉNDICES (SECCIÓN NUEVA SUGERIDA PARA ORGANIZAR FIGURAS ADICIONALES) ---

\appendix
\section{Apéndice}
\subsection{Análisis de Sensibilidad del Operador}
\label{sec:heatmap}
Para confirmar la robustez del ORT, se realizó un análisis de sensibilidad variando el factor de refuerzo ($\alpha$) y el percentil de selección del núcleo. La Figura~\ref{fig:appendix_heatmap} demuestra que el efecto del operador es estable y predecible.

\begin{figure}[h]
    \centering
    % Aquí insertas la figura del heatmap
    \includegraphics[width=0.6\textwidth]{sensitivity_heatmap.png}
    \caption{Heatmap de sensibilidad del ORT en el dataset Cora.}
    \label{fig:appendix_heatmap}
\end{figure}


\subsection{Análisis Preliminar en Conectoma de Macaco}
Como prueba de concepto de la aplicabilidad a datos biológicos, se realizó un análisis de centralidad sobre un conectoma del córtex del macaco Rhesus. La Figura~\ref{fig:appendix_macaque} muestra la distribución de activaciones tras la aplicación del ORT.

\begin{figure}[h]
    \centering
    % Aquí insertas la figura del histograma del macaco
    \includegraphics[width=0.6\textwidth]{macaque_activation_dist.png}
    \caption{Distribución de activaciones en el conectoma de macaco tras aplicar el ORT.}
    \label{fig:appendix_macaque}
\end{figure}

\newpage

% --- BIBLIOGRAFÍA (SIN CAMBIOS, YA ES MUY COMPLETA) ---
\begin{thebibliography}{9}

\bibitem{cora}
Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., \& Eliassi-Rad, T. (2008). Collective classification in network data. \textit{AI magazine}, 29(3), 93-106.

\bibitem{josselyn2020}
Josselyn, S. A., \& Tonegawa, S. (2020). Memory engrams: Recalling the past and imagining the future. \textit{Science}, 367(6473), eaaw4325.

\bibitem{hebb1949}
Hebb, D. O. (1949). \textit{The Organization of Behavior: A Neuropsychological Theory}. Wiley \& Sons.

\bibitem{bronstein2017}
Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \& Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. \textit{IEEE Signal Processing Magazine}, 34(4), 18-42.

\bibitem{semon1921}
Semon, R. (1921). \textit{The Mneme}. George Allen \& Unwin.

\bibitem{kipf2017}
Kipf, T. N., \& Welling, M. (2017). Semi-supervised classification with graph convolutional networks. \textit{Proceedings of the International Conference on Learning Representations (ICLR)}.

\bibitem{fey2019}
Fey, M., \& Lenssen, J. E. (2019). Fast graph representation learning with PyTorch Geometric. \textit{arXiv preprint arXiv:1903.02428}.

\bibitem{newman2018}
Newman, M. E. J. (2018). \textit{Networks}. Oxford university press.

\bibitem{petersen2018}
Petersen, P. C., \& Buzsáki, G. (2020). Cooling the brain: how crystallized intelligence forms. \textit{Trends in Cognitive Sciences}, 24(12), 979-989.

\bibitem{chen2022graphmemory}
Chen, H., Li, Z., \& Song, Y. (2022). Graph Memory Networks for Reasoning on Graphs. \textit{Proceedings of the AAAI Conference on Artificial Intelligence}, 36(7), 6543–6551.

\bibitem{kong2022graphtransformer}
Kong, L., Wang, H., \& Yu, P. S. (2022). Graph Transformer with Memory Augmentation. \textit{Proceedings of the Web Conference (WWW)}, 1245–1256.

\bibitem{rossi2023memgnn}
Rossi, E., Bianchi, F. M., \& Liò, P. (2023). Memory-augmented Graph Neural Networks. \textit{IEEE Transactions on Neural Networks and Learning Systems}, 34(5), 2101–2113.

\bibitem{roy2017engram}
Roy, D. S., Park, Y. G., Ogawa, S. K., \& Tonegawa, S. (2017). Memory retrieval by activating engram cells in mouse models of early Alzheimer’s disease. \textit{Nature}, 531(7595), 508–512.

\bibitem{ramirez2013creating}
Ramirez, S., Liu, X., Lin, P. A., Suh, J., Pignatelli, M., Redondo, R. L., Ryan, T. J., \& Tonegawa, S. (2013). Creating a false memory in the hippocampus. \textit{Science}, 341(6144), 387–391.

\bibitem{nr-aaai15}
Rossi, R. A., \& Ahmed, N. K. (2015). The Network Data Repository with Interactive Graph Analytics and Visualization. In \textit{Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence}. Recuperado de \url{http://networkrepository.com}.

\bibitem{academia12}
  title = {Graph Foundation Models: A Comprehensive Survey},
  author = {Wang, Zehong and Liu, Zheyuan and Ma, Tianyi and Li, Jiazheng and ...},
  journal = {arXiv preprint arXiv:2505.15116},
  year = {2025}


\bibitem{academia13}
  title = {GraphFM: A Comprehensive Benchmark for Graph Foundation Model},
  author = {Xu, Yuhao and Liu, Xinqi and Duan, Keyu and ...},
  journal = {arXiv preprint arXiv:2406.08310},
  year = {2024}

\end{thebibliography}

\end{document}

